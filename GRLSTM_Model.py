# import logging
# import os
# from collections import defaultdict
# from typing import Dict, Optional, Tuple, List

# import numpy as np
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from Data_Loader import load_semantic_info


# class PositionalEncoding(nn.Module):
#     def __init__(self, d_model: int, max_len: int = 20000):
#         super().__init__()
#         pe = torch.zeros(max_len, d_model)
#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
#         div_term = torch.exp(
#             torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)
#         )
#         pe[:, 0::2] = torch.sin(position * div_term)
#         pe[:, 1::2] = torch.cos(position * div_term)
#         pe = pe.unsqueeze(0)
#         self.register_buffer("pe", pe)

#     def forward(self, x: torch.Tensor) -> torch.Tensor:
#         time_steps = x.size(1)
#         if time_steps > self.pe.size(1):
#             self._expand_pe(time_steps, x.device)
#         return x + self.pe[:, :time_steps, :]

#     def _expand_pe(self, new_len: int, device):
#         d_model = self.pe.size(2)
#         pe = torch.zeros(new_len, d_model, device=device)
#         position = torch.arange(0, new_len, dtype=torch.float, device=device).unsqueeze(1)
#         div_term = torch.exp(
#             torch.arange(0, d_model, 2, device=device).float() * (-np.log(10000.0) / d_model)
#         )
#         pe[:, 0::2] = torch.sin(position * div_term)
#         pe[:, 1::2] = torch.cos(position * div_term)
#         self.pe = pe.unsqueeze(0)


# class OnlineTrajectoryAugmentor(nn.Module):
#     """
#     ğŸ”¥ åœ¨çº¿è½¨è¿¹å¢å¼ºå™¨ (åœ¨GPUä¸ŠåŠ¨æ€å¢å¼º)
    
#     ä¼˜åŠ¿:
#     - æ¯æ¬¡epochç”Ÿæˆä¸åŒçš„å¢å¼º (æ›´å¤šæ ·æ€§)
#     - æ— éœ€é¢„å…ˆåœ¨Data Loaderç”Ÿæˆ (èŠ‚çœå†…å­˜)
#     - æ”¯æŒå¯å­¦ä¹ çš„å¢å¼ºç­–ç•¥
#     """
    
#     def __init__(
#         self, 
#         topk_graph: Dict[int, List[Tuple[int, int]]],
#         category_lookup: torch.Tensor,  # [num_nodes]
#         num_nodes: int,
#         device: torch.device
#     ):
#         super().__init__()
#         self.num_nodes = num_nodes
#         self.device = device
        
#         # è½¬æ¢å›¾ç»“æ„ä¸ºtensor (æ–¹ä¾¿GPUæ“ä½œ)
#         self.graph_neighbors, self.graph_mask = self._build_graph_tensor(topk_graph, device)
        
#         # ç±»åˆ«æŸ¥æ‰¾è¡¨
#         self.register_buffer("category_lookup", category_lookup)
        
#         # æ„å»ºåŒç±»åˆ«POIæ˜ å°„
#         self.category_to_pois = self._build_category_mapping(category_lookup)
    
#     def _build_graph_tensor(self, topk_graph: Dict, device: torch.device):
#         """
#         å°†å›¾ç»“æ„è½¬ä¸ºtensor
#         è¿”å›: 
#             neighbors: [num_nodes, max_neighbors] POIé‚»å±…ID
#             mask: [num_nodes, max_neighbors] æœ‰æ•ˆé‚»å±…mask
#         """
#         max_neighbors = max(len(v) for v in topk_graph.values()) if topk_graph else 1
        
#         neighbors = torch.zeros(self.num_nodes, max_neighbors, dtype=torch.long, device=device)
#         mask = torch.zeros(self.num_nodes, max_neighbors, dtype=torch.bool, device=device)
        
#         for node_id, neighbor_list in topk_graph.items():
#             for i, (nb_id, _) in enumerate(neighbor_list):
#                 neighbors[node_id, i] = nb_id
#                 mask[node_id, i] = True
        
#         return neighbors, mask
    
#     def _build_category_mapping(self, category_lookup: torch.Tensor):
#         """æ„å»º {ç±»åˆ«: [POIåˆ—è¡¨]} æ˜ å°„"""
#         category_to_pois = defaultdict(list)
#         for poi_id in range(self.num_nodes):
#             cat = int(category_lookup[poi_id].item())
#             category_to_pois[cat].append(poi_id)
        
#         # è½¬ä¸ºtensor
#         for cat in category_to_pois:
#             category_to_pois[cat] = torch.tensor(
#                 category_to_pois[cat], 
#                 dtype=torch.long, 
#                 device=self.device
#             )
        
#         return category_to_pois
    
#     def neighbor_substitution(self, traj: torch.Tensor, ratio: float = 0.3) -> torch.Tensor:
#         """
#         ç©ºé—´å¢å¼º: é‚»å±…æ›¿æ¢
        
#         traj: [B, T] è½¨è¿¹å¼ é‡
#         è¿”å›: [B, T] å¢å¼ºåçš„è½¨è¿¹
#         """
#         B, T = traj.shape
#         traj_aug = traj.clone()
        
#         # éšæœºé€‰æ‹©è¦æ›¿æ¢çš„ä½ç½®
#         num_replace = max(1, int(T * ratio))
        
#         for b in range(B):
#             # æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹éšæœº
#             replace_indices = torch.randperm(T, device=self.device)[:num_replace]
            
#             for idx in replace_indices:
#                 poi = int(traj[b, idx].item())
                
#                 # è·å–é‚»å±…
#                 neighbors = self.graph_neighbors[poi]
#                 neighbor_mask = self.graph_mask[poi]
                
#                 valid_neighbors = neighbors[neighbor_mask]
                
#                 if valid_neighbors.numel() > 0:
#                     # éšæœºé€‰æ‹©ä¸€ä¸ªé‚»å±…
#                     random_idx = torch.randint(0, valid_neighbors.numel(), (1,), device=self.device)
#                     traj_aug[b, idx] = valid_neighbors[random_idx]
        
#         return traj_aug
    
#     def poi_generalization(self, traj: torch.Tensor, ratio: float = 0.2) -> torch.Tensor:
#         """
#         ç©ºé—´å¢å¼º: POIæ³›åŒ– (æ›¿æ¢ä¸ºåŒç±»åˆ«POI)
#         """
#         B, T = traj.shape
#         traj_aug = traj.clone()
        
#         num_replace = max(1, int(T * ratio))
        
#         for b in range(B):
#             replace_indices = torch.randperm(T, device=self.device)[:num_replace]
            
#             for idx in replace_indices:
#                 poi = int(traj[b, idx].item())
#                 category = int(self.category_lookup[poi].item())
                
#                 # åŒç±»åˆ«POI
#                 same_category_pois = self.category_to_pois.get(category, None)
                
#                 if same_category_pois is not None and same_category_pois.numel() > 1:
#                     # æ’é™¤è‡ªå·±
#                     candidates = same_category_pois[same_category_pois != poi]
                    
#                     if candidates.numel() > 0:
#                         random_idx = torch.randint(0, candidates.numel(), (1,), device=self.device)
#                         traj_aug[b, idx] = candidates[random_idx]
        
#         return traj_aug
    
#     def temporal_subsampling(self, traj: torch.Tensor, ratio: float = 0.7) -> torch.Tensor:
#         """
#         æ—¶é—´å¢å¼º: å­åºåˆ—é‡‡æ ·
        
#         è¿”å›: [B, T'] é•¿åº¦å¯èƒ½å˜åŒ–
#         """
#         B, T = traj.shape
#         keep_length = max(2, int(T * ratio))
        
#         # ç®€åŒ–ç‰ˆæœ¬: æ‰€æœ‰æ ·æœ¬ç»Ÿä¸€é•¿åº¦
#         traj_aug = []
        
#         for b in range(B):
#             # éšæœºèµ·å§‹ä½ç½®
#             if T > keep_length:
#                 start = torch.randint(0, T - keep_length + 1, (1,), device=self.device).item()
#                 traj_aug.append(traj[b, start:start + keep_length])
#             else:
#                 traj_aug.append(traj[b])
        
#         # Padåˆ°ç»Ÿä¸€é•¿åº¦
#         return torch.nn.utils.rnn.pad_sequence(
#             traj_aug, batch_first=True, padding_value=0
#         )
    
#     def temporal_resampling(self, traj: torch.Tensor, scale: float = 0.8) -> torch.Tensor:
#         """
#         æ—¶é—´å¢å¼º: é‡é‡‡æ ·
        
#         scale < 1: å‹ç¼© (åˆ é™¤ä¸€äº›POI)
#         scale > 1: æ‹‰ä¼¸ (é‡å¤ä¸€äº›POI)
#         """
#         B, T = traj.shape
#         new_T = max(2, int(T * scale))
        
#         # ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼
#         old_indices = torch.linspace(0, T - 1, T, device=self.device)
#         new_indices = torch.linspace(0, T - 1, new_T, device=self.device)
        
#         traj_aug = []
#         for b in range(B):
#             # æ’å€¼
#             resampled = []
#             for idx in new_indices:
#                 nearest = int(torch.round(idx).item())
#                 resampled.append(traj[b, nearest])
#             traj_aug.append(torch.stack(resampled))
        
#         return torch.stack(traj_aug)
    
#     def forward(self, traj: torch.Tensor, aug_type: str = 'spatial') -> torch.Tensor:
#         """
#         å‰å‘ä¼ æ’­: åº”ç”¨å¢å¼º
        
#         aug_type: 'spatial' / 'temporal' / 'random'
#         """
#         if aug_type == 'random':
#             aug_type = np.random.choice(['spatial', 'temporal'])
        
#         if aug_type == 'spatial':
#             # éšæœºé€‰æ‹©ç©ºé—´å¢å¼ºç­–ç•¥
#             strategy = np.random.choice(['neighbor', 'generalize'])
#             if strategy == 'neighbor':
#                 return self.neighbor_substitution(traj)
#             else:
#                 return self.poi_generalization(traj)
        
#         else:  # temporal
#             # éšæœºé€‰æ‹©æ—¶é—´å¢å¼ºç­–ç•¥
#             strategy = np.random.choice(['subsample', 'resample'])
#             if strategy == 'subsample':
#                 return self.temporal_subsampling(traj)
#             else:
#                 scale = np.random.uniform(0.7, 1.3)
#                 return self.temporal_resampling(traj, scale)


# class GRLSTM(nn.Module):
#     """
#     ğŸ”¥ é›†æˆåœ¨çº¿å¢å¼ºå™¨çš„GRLSTM
#     """

#     def __init__(self, args, device, batch_first=True):
#         super().__init__()
#         self.nodes = args.nodes
#         self.latent_dim = args.latent_dim
#         self.device = device
#         self.batch_first = batch_first
#         self.num_heads = args.num_heads
#         self.topk_neighbors = args.topk_neighbors
#         self.self_loop_rel = 9
#         self.rel_vocab = self.self_loop_rel + 1

#         logging.info("Initializing model: latent_dim=%d", self.latent_dim)

#         # Fixed fusion weights
#         alpha_spa = float(getattr(args, "alpha_spa", 0.3))
#         delta_sem = float(getattr(args, "delta_sem", 0.5))
#         s = max(alpha_spa + delta_sem, 1e-8)
#         w_spa = alpha_spa / s
#         w_sem = delta_sem / s

#         self.register_buffer("w_sem", torch.tensor(w_sem, dtype=torch.float32))
#         self.register_buffer("w_spa", torch.tensor(w_spa, dtype=torch.float32))
#         logging.info("Fixed fusion weights: w_sem=%.4f, w_spa=%.4f", w_sem, w_spa)

#         # Semantic meta
#         category_lookup, subclass_lookup, cat_vocab, sub_vocab = load_semantic_info(
#             args.semantic_file, args.nodes
#         )
#         args.category_vocab = getattr(args, "category_vocab", cat_vocab)
#         args.subclass_vocab = getattr(args, "subclass_vocab", sub_vocab)
#         self.register_buffer("category_lookup", torch.as_tensor(category_lookup, dtype=torch.long))
#         self.register_buffer("subclass_lookup", torch.as_tensor(subclass_lookup, dtype=torch.long))

#         # Spatial branch
#         neighbors_np = np.load(args.poi_file, allow_pickle=True)["neighbors"]
#         self.topk_graph = self._build_topk_graph(args.kg_multi_rel_file, neighbors_np)

#         poi_features = np.load(args.poi_feature_file)
#         self.node_embedding = nn.Embedding.from_pretrained(
#             torch.tensor(poi_features, dtype=torch.float32),
#             freeze=False,
#         )
#         self.rel_embedding = nn.Embedding(self.rel_vocab, self.latent_dim)

#         self._precompute_rel_bias()
#         self.spatial_pos_enc = PositionalEncoding(self.latent_dim, max_len=20000)

#         # Semantic branch
#         self.token_embedding = nn.Embedding(args.subclass_vocab, self.latent_dim, padding_idx=0)
#         self.segment_embedding = nn.Embedding(args.category_vocab, self.latent_dim, padding_idx=0)
#         self.pos_embedding = nn.Embedding(args.max_seq_len, self.latent_dim)
#         self.max_seq_len = args.max_seq_len

#         sem_encoder_layer = nn.TransformerEncoderLayer(
#             d_model=self.latent_dim,
#             nhead=args.num_heads,
#             dim_feedforward=args.trans_ffn_dim,
#             dropout=args.trans_dropout,
#             batch_first=True,
#         )
#         self.semantic_encoder = nn.TransformerEncoder(
#             sem_encoder_layer, num_layers=args.trans_layers
#         )

#         # Dual cross-attn
#         self.co_attn_sem = nn.MultiheadAttention(
#             embed_dim=self.latent_dim,
#             num_heads=getattr(args, "co_heads", args.num_heads),
#             batch_first=True,
#             dropout=args.trans_dropout,
#         )
#         self.co_attn_spa = nn.MultiheadAttention(
#             embed_dim=self.latent_dim,
#             num_heads=getattr(args, "co_heads", args.num_heads),
#             batch_first=True,
#             dropout=args.trans_dropout,
#         )

#         self.fuse_linear = nn.Linear(self.latent_dim * 2, self.latent_dim)
#         self.fuse_norm = nn.LayerNorm(self.latent_dim)
#         self.fuse_dropout = nn.Dropout(args.trans_dropout)

#         # FFN block
#         self.fusion_ffn = nn.Sequential(
#             nn.Linear(self.latent_dim, args.trans_ffn_dim),
#             nn.GELU(),
#             nn.Dropout(args.trans_dropout),
#             nn.Linear(args.trans_ffn_dim, self.latent_dim),
#             nn.Dropout(args.trans_dropout),
#         )
#         self.fusion_ffn_norm = nn.LayerNorm(self.latent_dim)
        
#         # ğŸ”¥ åœ¨çº¿å¢å¼ºå™¨ (å¯é€‰)
#         self.use_online_aug = getattr(args, 'use_online_augmentation', False)
#         if self.use_online_aug:
#             self.augmentor = OnlineTrajectoryAugmentor(
#                 topk_graph=self.topk_graph,
#                 category_lookup=self.category_lookup,
#                 num_nodes=self.nodes,
#                 device=device
#             )
#             logging.info("Online trajectory augmentation enabled")

#     def _build_topk_graph(self, kg_multi_rel_file: str, neighbors_np: np.ndarray):
#         graph: Dict[int, List[Tuple[int, int]]] = defaultdict(list)

#         if os.path.exists(kg_multi_rel_file):
#             with open(kg_multi_rel_file, "r") as f:
#                 for line in f:
#                     if not line.strip():
#                         continue
#                     try:
#                         u, v, r = line.strip().split()
#                         u, v, r = int(u), int(v), int(r)
#                     except ValueError:
#                         continue
#                     graph[u].append((v, r))
#                     graph[v].append((u, r))

#         for nid in range(self.nodes):
#             candidates = graph.get(nid, [])
#             for nb in neighbors_np[nid]:
#                 candidates.append((int(nb), 0))

#             seen = set()
#             top = []
#             for nb, rel in candidates:
#                 if nb in seen:
#                     continue
#                 seen.add(nb)
#                 top.append((nb, rel))
#                 if len(top) >= self.topk_neighbors:
#                     break
#             graph[nid] = top
#         return graph

#     def _build_padding_mask(self, lengths, max_len, device: Optional[torch.device] = None):
#         dev = device if device is not None else self.rel_bias_cache.device
#         lens = torch.as_tensor(lengths, device=dev)
#         return torch.arange(max_len, device=dev).unsqueeze(0) >= lens.unsqueeze(1)

#     def _masked_avg_pool(self, seq: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:
#         valid = (~padding_mask).float().unsqueeze(-1)
#         seq = seq * valid
#         denom = valid.sum(dim=1).clamp(min=1.0)
#         return seq.sum(dim=1) / denom

#     def _precompute_rel_bias(self):
#         logging.info("[Model] Precomputing rel_bias for %d nodes...", self.nodes)
#         rel_bias = torch.zeros((self.nodes, self.latent_dim), dtype=torch.float32)

#         with torch.no_grad():
#             rel_w = self.rel_embedding.weight.detach().cpu()
#             for nid in range(self.nodes):
#                 neighbors = self.topk_graph.get(nid, [])
#                 if not neighbors:
#                     continue
#                 rel_ids = [rel for _, rel in neighbors]
#                 if len(rel_ids) > 0:
#                     rel_ids_t = torch.tensor(rel_ids, dtype=torch.long)
#                     rel_bias[nid] = rel_w[rel_ids_t].mean(dim=0)

#         self.register_buffer("rel_bias_cache", rel_bias, persistent=True)
#         assert not self.rel_bias_cache.requires_grad
#         logging.info("[Model] Precomputed rel_bias: shape=%s", rel_bias.shape)

#     def _encode_spatial(
#         self,
#         nodes: torch.Tensor,
#         lengths,
#         poi: Optional[torch.Tensor] = None,
#         traj_poi: Optional[torch.Tensor] = None,
#         mask: Optional[torch.Tensor] = None,
#         poi_lengths: Optional[List[int]] = None,
#         traj_poi_lengths: Optional[List[int]] = None,
#     ):
#         dev = nodes.device

#         if mask is None:
#             mask = self._build_padding_mask(lengths, nodes.size(1), device=dev)

#         valid = nodes.masked_select(~mask)

#         if poi is not None:
#             if poi_lengths is not None:
#                 poi_mask = self._build_padding_mask(poi_lengths, poi.size(1), device=poi.device)
#             else:
#                 poi_mask = torch.zeros_like(poi, dtype=torch.bool, device=poi.device)
#             valid = torch.cat([valid, poi.masked_select(~poi_mask)], dim=0)

#         if traj_poi is not None:
#             if traj_poi_lengths is not None:
#                 traj_mask = self._build_padding_mask(traj_poi_lengths, traj_poi.size(1), device=traj_poi.device)
#             else:
#                 traj_mask = torch.zeros_like(traj_poi, dtype=torch.bool, device=traj_poi.device)
#             valid = torch.cat([valid, traj_poi.masked_select(~traj_mask)], dim=0)

#         unique_nodes = torch.unique(valid)
#         if unique_nodes.numel() == 0:
#             unique_nodes = torch.tensor([0], device=dev, dtype=torch.long)

#         edges: List[Tuple[int, int, int]] = []
#         for nid in unique_nodes.tolist():
#             edges.append((nid, nid, self.self_loop_rel))
#             for nb, rel in self.topk_graph.get(nid, []):
#                 edges.append((nb, nid, rel))

#         if not edges:
#             edges = [(0, 0, self.self_loop_rel)]

#         src = torch.as_tensor([e[0] for e in edges], device=dev, dtype=torch.long)
#         dst = torch.as_tensor([e[1] for e in edges], device=dev, dtype=torch.long)
#         rel = torch.as_tensor([e[2] for e in edges], device=dev, dtype=torch.long)

#         node_feats = self.node_embedding.weight.to(dev) + self.rel_bias_cache.to(dev)
#         rel_emb = self.rel_embedding(rel)
#         messages = node_feats[src] + rel_emb

#         agg = torch.zeros_like(node_feats)
#         agg.index_add_(0, dst, messages)

#         deg = torch.zeros(self.nodes, device=dev)
#         deg.index_add_(0, dst, torch.ones(len(dst), device=dev))
#         deg = deg.clamp(min=1).unsqueeze(-1)
#         agg = agg / deg

#         spatial_features = agg + node_feats
#         spatial_features = F.normalize(spatial_features, p=2, dim=-1) * np.sqrt(self.latent_dim)

#         spatial_seq = spatial_features[nodes]
#         poi_seq = spatial_features[poi] if poi is not None else None
#         traj_poi_seq = spatial_features[traj_poi] if traj_poi is not None else None

#         return spatial_seq, poi_seq, traj_poi_seq

#     def _semantic_from_nodes(self, nodes: torch.Tensor, lengths):
#         positions = torch.arange(nodes.size(1), device=nodes.device).unsqueeze(0).expand(nodes.size(0), -1)
#         return {
#             "tokens": self.subclass_lookup[nodes],
#             "segments": self.category_lookup[nodes],
#             "positions": positions,
#             "padding_mask": self._build_padding_mask(lengths, nodes.size(1), device=nodes.device),
#             "lengths": lengths,
#         }

#     def encode_poi_batch(self, poi_batch: torch.Tensor, poi_lengths):
#         poi_batch = poi_batch.to(self.device)
#         mask = self._build_padding_mask(poi_lengths, poi_batch.size(1), device=poi_batch.device)
#         spatial_seq, _, _ = self._encode_spatial(
#             poi_batch, poi_lengths, poi=None, traj_poi=None, mask=mask
#         )
#         spatial_seq = self.spatial_pos_enc(spatial_seq)
#         return self._masked_avg_pool(spatial_seq, mask)
    
#     def generate_augmented_views(
#         self, 
#         batch_nodes: torch.Tensor, 
#         batch_lengths
#     ) -> Tuple[torch.Tensor, torch.Tensor, List, List]:
#         """
#         ğŸ”¥ åœ¨çº¿ç”Ÿæˆå¢å¼ºè§†å›¾
        
#         è¿”å›:
#             spatial_aug: [B, T] ç©ºé—´å¢å¼ºè½¨è¿¹
#             temporal_aug: [B, T'] æ—¶é—´å¢å¼ºè½¨è¿¹
#             spatial_aug_lengths: ç©ºé—´å¢å¼ºé•¿åº¦
#             temporal_aug_lengths: æ—¶é—´å¢å¼ºé•¿åº¦
#         """
#         if not self.use_online_aug:
#             # è¿”å›åŸå§‹è½¨è¿¹çš„å‰¯æœ¬
#             return batch_nodes.clone(), batch_nodes.clone(), batch_lengths, batch_lengths
        
#         # ç”Ÿæˆä¸¤ä¸ªå¢å¼ºè§†å›¾
#         spatial_aug = self.augmentor(batch_nodes, aug_type='spatial')
#         temporal_aug = self.augmentor(batch_nodes, aug_type='temporal')
        
#         # è®¡ç®—æ–°é•¿åº¦ (temporalå¯èƒ½æ”¹å˜é•¿åº¦)
#         spatial_aug_lengths = batch_lengths  # ç©ºé—´å¢å¼ºä¸æ”¹å˜é•¿åº¦
#         temporal_aug_lengths = [(temporal_aug[i] != 0).sum().item() for i in range(temporal_aug.size(0))]
        
#         return spatial_aug, temporal_aug, spatial_aug_lengths, temporal_aug_lengths

#     def forward(
#         self,
#         batch_nodes: torch.Tensor,
#         batch_lengths,
#         semantic: Optional[Dict[str, torch.Tensor]] = None,
#         poi: Optional[torch.Tensor] = None,
#         traj_poi: Optional[torch.Tensor] = None,
#         poi_lengths: Optional[List[int]] = None,
#         traj_poi_lengths: Optional[List[int]] = None,
#         return_augmented_views: bool = False,  # ğŸ”¥ æ˜¯å¦è¿”å›å¢å¼ºè§†å›¾
#     ):
#         batch_nodes = batch_nodes.to(self.device)
#         mask = self._build_padding_mask(batch_lengths, batch_nodes.size(1), device=batch_nodes.device)

#         spatial_seq, poi_seq, traj_poi_seq = self._encode_spatial(
#             batch_nodes, batch_lengths, poi, traj_poi, mask,
#             poi_lengths=poi_lengths, traj_poi_lengths=traj_poi_lengths,
#         )
#         spatial_seq = self.spatial_pos_enc(spatial_seq)

#         if semantic is None:
#             semantic = self._semantic_from_nodes(batch_nodes, batch_lengths)

#         tokens = semantic["tokens"].to(batch_nodes.device)
#         segments = semantic["segments"].to(batch_nodes.device)
#         positions = semantic["positions"].to(batch_nodes.device)
#         sem_mask = semantic.get("padding_mask", mask).to(batch_nodes.device)

#         positions = positions.clamp(max=self.pos_embedding.num_embeddings - 1)

#         sem_input = self.token_embedding(tokens) + self.segment_embedding(segments)
#         sem_input = sem_input + self.pos_embedding(positions)

#         semantic_out = self.semantic_encoder(sem_input, src_key_padding_mask=sem_mask)

#         sem_attn, _ = self.co_attn_sem(
#             semantic_out, spatial_seq, spatial_seq, key_padding_mask=mask,
#         )
#         spa_attn, _ = self.co_attn_spa(
#             spatial_seq, semantic_out, semantic_out, key_padding_mask=sem_mask,
#         )

#         combined = torch.cat([sem_attn, spa_attn], dim=-1)
#         weighted = self.w_sem * sem_attn + self.w_spa * spa_attn

#         fused = self.fuse_linear(combined)
#         fused = self.fuse_norm(fused + self.fuse_dropout(weighted))

#         ffn_out = self.fusion_ffn(fused)
#         fused = self.fusion_ffn_norm(fused + ffn_out)

#         traj_repr = self._masked_avg_pool(fused, mask)

#         poi_emb_pooled = None
#         traj_poi_emb_pooled = None

#         if poi_seq is not None:
#             if poi_lengths is None:
#                 poi_mask = mask
#             else:
#                 poi_mask = self._build_padding_mask(poi_lengths, poi_seq.size(1), device=poi_seq.device)
#             poi_emb_pooled = self._masked_avg_pool(poi_seq, poi_mask)

#         if traj_poi_seq is not None:
#             if traj_poi_lengths is None:
#                 traj_poi_mask = mask
#             else:
#                 traj_poi_mask = self._build_padding_mask(traj_poi_lengths, traj_poi_seq.size(1), device=traj_poi_seq.device)
#             traj_poi_emb_pooled = self._masked_avg_pool(traj_poi_seq, traj_poi_mask)
#         output = {
#             "traj_repr": traj_repr,
#             "poi_emb": poi_emb_pooled,
#             "traj_poi_emb": traj_poi_emb_pooled,
#             "fused_seq": fused,
#             "mask": mask,
#         }
    
#     # ğŸ”¥ å¦‚æœéœ€è¦,ç”Ÿæˆå¹¶è¿”å›å¢å¼ºè§†å›¾
#         if return_augmented_views and self.use_online_aug:
#             spatial_aug, temporal_aug, spa_len, temp_len = self.generate_augmented_views(
#                 batch_nodes, batch_lengths
#             )
#             output["spatial_aug"] = spatial_aug
#             output["temporal_aug"] = temporal_aug
#             output["spatial_aug_lengths"] = spa_len
#             output["temporal_aug_lengths"] = temp_len

#         return output


# # ä¸¤è§†å›¾
import logging
import os
from collections import defaultdict
from typing import Dict, Optional, Tuple, List

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from Data_Loader import load_semantic_info


class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 20000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        time_steps = x.size(1)
        if time_steps > self.pe.size(1):
            self._expand_pe(time_steps, x.device)
        return x + self.pe[:, :time_steps, :]

    def _expand_pe(self, new_len: int, device):
        d_model = self.pe.size(2)
        pe = torch.zeros(new_len, d_model, device=device)
        position = torch.arange(0, new_len, dtype=torch.float, device=device).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2, device=device).float() * (-np.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.pe = pe.unsqueeze(0)


class TwoViewGCL(nn.Module):
    """
    ä¸¤è§†å›¾å›¾å¯¹æ¯”å­¦ä¹ æ¨¡å—
    
    è§†å›¾åˆ’åˆ†:
    - è§†å›¾1 (ç©ºé—´-é“è·¯): road(0), highway(7), non_highway(8)  
    - è§†å›¾2 (æ—¶é—´-è½¨è¿¹): traj_in(1), traj_not_in(2), weekday(3), weekend(4), peak(5), offpeak(6)
    """
    
    def __init__(self, latent_dim: int, temperature: float = 0.07):
        super().__init__()
        self.latent_dim = latent_dim
        self.temperature = temperature
        
        # ğŸ”¥ å…³é”®: å®šä¹‰ä¸¤ä¸ªè§†å›¾çš„å…³ç³»ç±»å‹
        self.VIEW1_RELS = [0, 7, 8]              # ç©ºé—´-é“è·¯è§†å›¾
        self.VIEW2_RELS = [1, 2, 3, 4, 5, 6]     # æ—¶é—´-è½¨è¿¹è§†å›¾
        
        # æŠ•å½±å¤´: å°†latent_dimé™ç»´åˆ°latent_dim//2ç”¨äºå¯¹æ¯”å­¦ä¹ 
        self.proj_view1 = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim // 2)
        )
        self.proj_view2 = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim // 2)
        )
    
    def _filter_graph(
        self, 
        full_graph: Dict[int, List[Tuple[int, int]]],
        allowed_rels: List[int]
    ) -> Dict[int, List[Tuple[int, int]]]:
        """æ ¹æ®å…è®¸çš„å…³ç³»ç±»å‹è¿‡æ»¤å›¾ç»“æ„"""
        filtered = defaultdict(list)
        for node_id, neighbors in full_graph.items():
            for neighbor_id, relation_type in neighbors:
                if relation_type in allowed_rels:
                    filtered[node_id].append((neighbor_id, relation_type))
        return filtered
    
    def _aggregate_view(
        self,
        node_features: torch.Tensor,      # [num_nodes, D]
        subgraph: Dict[int, List[Tuple[int, int]]],
        rel_embedding: nn.Embedding,
        batch_node_ids: torch.Tensor,      # [B]
        device: torch.device
    ) -> torch.Tensor:
        """
        åœ¨å­å›¾ä¸Šèšåˆé‚»å±…ç‰¹å¾
        è¿”å›: [B, D]
        """
        B = batch_node_ids.size(0)
        aggregated = torch.zeros(B, self.latent_dim, device=device)
        
        for i, node_id in enumerate(batch_node_ids.tolist()):
            neighbors = subgraph.get(node_id, [])
            
            if not neighbors:
                # æ²¡æœ‰é‚»å±…åˆ™ä½¿ç”¨è‡ªèº«ç‰¹å¾
                aggregated[i] = node_features[node_id]
                continue
            
            # æå–é‚»å±…IDå’Œå…³ç³»ç±»å‹
            nb_ids = torch.tensor([nb for nb, _ in neighbors], device=device, dtype=torch.long)
            rel_ids = torch.tensor([rel for _, rel in neighbors], device=device, dtype=torch.long)
            
            # å…³ç³»å¢å¼ºçš„æ¶ˆæ¯ä¼ é€’
            nb_feats = node_features[nb_ids]        # [K, D]
            rel_feats = rel_embedding(rel_ids)       # [K, D]
            messages = nb_feats + rel_feats
            
            # å¹³å‡èšåˆ
            aggregated[i] = messages.mean(dim=0)
        
        return aggregated
    
    def _infonce_loss(self, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:
        """
        InfoNCEå¯¹æ¯”æŸå¤±
        z1, z2: [B, D'] ä¸¤ä¸ªè§†å›¾çš„æŠ•å½±
        """
        B = z1.size(0)
        
        # L2å½’ä¸€åŒ–
        z1 = F.normalize(z1, p=2, dim=-1)
        z2 = F.normalize(z2, p=2, dim=-1)
        
        # ç›¸ä¼¼åº¦çŸ©é˜µ [B, B]
        sim = torch.mm(z1, z2.t()) / self.temperature
        
        # å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬
        labels = torch.arange(B, device=z1.device)
        
        # åŒå‘æŸå¤±
        loss_12 = F.cross_entropy(sim, labels)
        loss_21 = F.cross_entropy(sim.t(), labels)
        
        return (loss_12 + loss_21) / 2
    
    def forward(
        self,
        node_features: torch.Tensor,
        full_graph: Dict[int, List[Tuple[int, int]]],
        rel_embedding: nn.Embedding,
        batch_node_ids: torch.Tensor,
        device: torch.device
    ) -> Tuple[torch.Tensor, Dict]:
        """
        å‰å‘ä¼ æ’­
        è¿”å›: (loss, info_dict)
        """
        # ğŸ”¥ Step 1: è¿‡æ»¤å‡ºä¸¤ä¸ªè§†å›¾
        view1_graph = self._filter_graph(full_graph, self.VIEW1_RELS)
        view2_graph = self._filter_graph(full_graph, self.VIEW2_RELS)
        
        # ğŸ”¥ Step 2: åœ¨ä¸¤ä¸ªè§†å›¾ä¸Šåˆ†åˆ«èšåˆ
        view1_agg = self._aggregate_view(node_features, view1_graph, rel_embedding, batch_node_ids, device)
        view2_agg = self._aggregate_view(node_features, view2_graph, rel_embedding, batch_node_ids, device)
        
        # ğŸ”¥ Step 3: æŠ•å½±
        z1 = self.proj_view1(view1_agg)
        z2 = self.proj_view2(view2_agg)
        
        # ğŸ”¥ Step 4: å¯¹æ¯”æŸå¤±
        loss = self._infonce_loss(z1, z2)
        
        info = {
            'gcl_loss': float(loss.item()),
            'view1_nodes': len(view1_graph),
            'view2_nodes': len(view2_graph),
        }
        
        return loss, info


class GRLSTM(nn.Module):
    """
    Dual-branch encoder with Two-View Graph Contrastive Learning
    """

    def __init__(self, args, device, batch_first=True):
        super().__init__()
        self.nodes = args.nodes
        self.latent_dim = args.latent_dim
        self.device = device
        self.batch_first = batch_first
        self.num_heads = args.num_heads
        self.topk_neighbors = args.topk_neighbors
        self.self_loop_rel = 9
        self.rel_vocab = self.self_loop_rel + 1

        logging.info("Initializing model: latent_dim=%d", self.latent_dim)

        # Fixed fusion weights
        alpha_spa = float(getattr(args, "alpha_spa", 0.3))
        delta_sem = float(getattr(args, "delta_sem", 0.5))
        s = max(alpha_spa + delta_sem, 1e-8)
        w_spa = alpha_spa / s
        w_sem = delta_sem / s

        self.register_buffer("w_sem", torch.tensor(w_sem, dtype=torch.float32))
        self.register_buffer("w_spa", torch.tensor(w_spa, dtype=torch.float32))
        logging.info("Fixed fusion weights: w_sem=%.4f, w_spa=%.4f", w_sem, w_spa)

        # Semantic meta
        category_lookup, subclass_lookup, cat_vocab, sub_vocab = load_semantic_info(
            args.semantic_file, args.nodes
        )
        args.category_vocab = getattr(args, "category_vocab", cat_vocab)
        args.subclass_vocab = getattr(args, "subclass_vocab", sub_vocab)
        self.register_buffer("category_lookup", torch.as_tensor(category_lookup, dtype=torch.long))
        self.register_buffer("subclass_lookup", torch.as_tensor(subclass_lookup, dtype=torch.long))

        # Spatial branch
        neighbors_np = np.load(args.poi_file, allow_pickle=True)["neighbors"]
        self.topk_graph = self._build_topk_graph(args.kg_multi_rel_file, neighbors_np)

        poi_features = np.load(args.poi_feature_file)
        self.node_embedding = nn.Embedding.from_pretrained(
            torch.tensor(poi_features, dtype=torch.float32),
            freeze=False,
        )
        self.rel_embedding = nn.Embedding(self.rel_vocab, self.latent_dim)

        self._precompute_rel_bias()
        self.spatial_pos_enc = PositionalEncoding(self.latent_dim, max_len=20000)

        # Semantic branch
        self.token_embedding = nn.Embedding(args.subclass_vocab, self.latent_dim, padding_idx=0)
        self.segment_embedding = nn.Embedding(args.category_vocab, self.latent_dim, padding_idx=0)
        self.pos_embedding = nn.Embedding(args.max_seq_len, self.latent_dim)
        self.max_seq_len = args.max_seq_len

        sem_encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.latent_dim,
            nhead=args.num_heads,
            dim_feedforward=args.trans_ffn_dim,
            dropout=args.trans_dropout,
            batch_first=True,
        )
        self.semantic_encoder = nn.TransformerEncoder(
            sem_encoder_layer, num_layers=args.trans_layers
        )

        # Dual cross-attn
        self.co_attn_sem = nn.MultiheadAttention(
            embed_dim=self.latent_dim,
            num_heads=getattr(args, "co_heads", args.num_heads),
            batch_first=True,
            dropout=args.trans_dropout,
        )
        self.co_attn_spa = nn.MultiheadAttention(
            embed_dim=self.latent_dim,
            num_heads=getattr(args, "co_heads", args.num_heads),
            batch_first=True,
            dropout=args.trans_dropout,
        )

        self.fuse_linear = nn.Linear(self.latent_dim * 2, self.latent_dim)
        self.fuse_norm = nn.LayerNorm(self.latent_dim)
        self.fuse_dropout = nn.Dropout(args.trans_dropout)

        # FFN block
        self.fusion_ffn = nn.Sequential(
            nn.Linear(self.latent_dim, args.trans_ffn_dim),
            nn.GELU(),
            nn.Dropout(args.trans_dropout),
            nn.Linear(args.trans_ffn_dim, self.latent_dim),
            nn.Dropout(args.trans_dropout),
        )
        self.fusion_ffn_norm = nn.LayerNorm(self.latent_dim)
        
        # ğŸ”¥ ä¸¤è§†å›¾å›¾å¯¹æ¯”å­¦ä¹ æ¨¡å—
        self.use_gcl = getattr(args, 'use_gcl', True)
        if self.use_gcl:
            gcl_temp = getattr(args, 'gcl_temperature', 0.07)
            self.gcl_module = TwoViewGCL(self.latent_dim, temperature=gcl_temp)
            logging.info("Two-view GCL enabled (temperature=%.3f)", gcl_temp)

    def _build_topk_graph(self, kg_multi_rel_file: str, neighbors_np: np.ndarray):
        graph: Dict[int, List[Tuple[int, int]]] = defaultdict(list)

        if os.path.exists(kg_multi_rel_file):
            with open(kg_multi_rel_file, "r") as f:
                for line in f:
                    if not line.strip():
                        continue
                    try:
                        u, v, r = line.strip().split()
                        u, v, r = int(u), int(v), int(r)
                    except ValueError:
                        continue
                    graph[u].append((v, r))
                    graph[v].append((u, r))

        for nid in range(self.nodes):
            candidates = graph.get(nid, [])
            for nb in neighbors_np[nid]:
                candidates.append((int(nb), 0))

            seen = set()
            top = []
            for nb, rel in candidates:
                if nb in seen:
                    continue
                seen.add(nb)
                top.append((nb, rel))
                if len(top) >= self.topk_neighbors:
                    break
            graph[nid] = top
        return graph

    def _build_padding_mask(self, lengths, max_len, device: Optional[torch.device] = None):
        dev = device if device is not None else self.rel_bias_cache.device
        lens = torch.as_tensor(lengths, device=dev)
        return torch.arange(max_len, device=dev).unsqueeze(0) >= lens.unsqueeze(1)

    def _masked_avg_pool(self, seq: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:
        valid = (~padding_mask).float().unsqueeze(-1)
        seq = seq * valid
        denom = valid.sum(dim=1).clamp(min=1.0)
        return seq.sum(dim=1) / denom

    def _precompute_rel_bias(self):
        logging.info("[Model] Precomputing rel_bias for %d nodes...", self.nodes)
        rel_bias = torch.zeros((self.nodes, self.latent_dim), dtype=torch.float32)

        with torch.no_grad():
            rel_w = self.rel_embedding.weight.detach().cpu()
            for nid in range(self.nodes):
                neighbors = self.topk_graph.get(nid, [])
                if not neighbors:
                    continue
                rel_ids = [rel for _, rel in neighbors]
                if len(rel_ids) > 0:
                    rel_ids_t = torch.tensor(rel_ids, dtype=torch.long)
                    rel_bias[nid] = rel_w[rel_ids_t].mean(dim=0)

        self.register_buffer("rel_bias_cache", rel_bias, persistent=True)
        assert not self.rel_bias_cache.requires_grad
        logging.info("[Model] Precomputed rel_bias: shape=%s", rel_bias.shape)

    def _encode_spatial(
        self,
        nodes: torch.Tensor,
        lengths,
        poi: Optional[torch.Tensor] = None,
        traj_poi: Optional[torch.Tensor] = None,
        mask: Optional[torch.Tensor] = None,
        poi_lengths: Optional[List[int]] = None,
        traj_poi_lengths: Optional[List[int]] = None,
    ):
        dev = nodes.device

        if mask is None:
            mask = self._build_padding_mask(lengths, nodes.size(1), device=dev)

        valid = nodes.masked_select(~mask)

        if poi is not None:
            if poi_lengths is not None:
                poi_mask = self._build_padding_mask(poi_lengths, poi.size(1), device=poi.device)
            else:
                poi_mask = torch.zeros_like(poi, dtype=torch.bool, device=poi.device)
            valid = torch.cat([valid, poi.masked_select(~poi_mask)], dim=0)

        if traj_poi is not None:
            if traj_poi_lengths is not None:
                traj_mask = self._build_padding_mask(traj_poi_lengths, traj_poi.size(1), device=traj_poi.device)
            else:
                traj_mask = torch.zeros_like(traj_poi, dtype=torch.bool, device=traj_poi.device)
            valid = torch.cat([valid, traj_poi.masked_select(~traj_mask)], dim=0)

        unique_nodes = torch.unique(valid)
        if unique_nodes.numel() == 0:
            unique_nodes = torch.tensor([0], device=dev, dtype=torch.long)

        edges: List[Tuple[int, int, int]] = []
        for nid in unique_nodes.tolist():
            edges.append((nid, nid, self.self_loop_rel))
            for nb, rel in self.topk_graph.get(nid, []):
                edges.append((nb, nid, rel))

        if not edges:
            edges = [(0, 0, self.self_loop_rel)]

        src = torch.as_tensor([e[0] for e in edges], device=dev, dtype=torch.long)
        dst = torch.as_tensor([e[1] for e in edges], device=dev, dtype=torch.long)
        rel = torch.as_tensor([e[2] for e in edges], device=dev, dtype=torch.long)

        node_feats = self.node_embedding.weight.to(dev) + self.rel_bias_cache.to(dev)
        rel_emb = self.rel_embedding(rel)
        messages = node_feats[src] + rel_emb

        agg = torch.zeros_like(node_feats)
        agg.index_add_(0, dst, messages)

        deg = torch.zeros(self.nodes, device=dev)
        deg.index_add_(0, dst, torch.ones(len(dst), device=dev))
        deg = deg.clamp(min=1).unsqueeze(-1)
        agg = agg / deg

        spatial_features = agg + node_feats
        spatial_features = F.normalize(spatial_features, p=2, dim=-1) * np.sqrt(self.latent_dim)

        spatial_seq = spatial_features[nodes]
        poi_seq = spatial_features[poi] if poi is not None else None
        traj_poi_seq = spatial_features[traj_poi] if traj_poi is not None else None

        return spatial_seq, poi_seq, traj_poi_seq

    def _semantic_from_nodes(self, nodes: torch.Tensor, lengths):
        positions = torch.arange(nodes.size(1), device=nodes.device).unsqueeze(0).expand(nodes.size(0), -1)
        return {
            "tokens": self.subclass_lookup[nodes],
            "segments": self.category_lookup[nodes],
            "positions": positions,
            "padding_mask": self._build_padding_mask(lengths, nodes.size(1), device=nodes.device),
            "lengths": lengths,
        }

    def encode_poi_batch(self, poi_batch: torch.Tensor, poi_lengths):
        poi_batch = poi_batch.to(self.device)
        mask = self._build_padding_mask(poi_lengths, poi_batch.size(1), device=poi_batch.device)
        spatial_seq, _, _ = self._encode_spatial(
            poi_batch, poi_lengths, poi=None, traj_poi=None, mask=mask
        )
        spatial_seq = self.spatial_pos_enc(spatial_seq)
        return self._masked_avg_pool(spatial_seq, mask)
    
    def compute_gcl_loss(self, batch_nodes: torch.Tensor, mask: torch.Tensor):
        """
        ğŸ”¥ è®¡ç®—ä¸¤è§†å›¾å›¾å¯¹æ¯”å­¦ä¹ æŸå¤±
        """
        if not self.use_gcl:
            return torch.tensor(0.0, device=batch_nodes.device), {}
        
        # æå–batchä¸­çš„å”¯ä¸€èŠ‚ç‚¹
        valid_nodes = batch_nodes.masked_select(~mask)
        unique_nodes = torch.unique(valid_nodes)
        
        if unique_nodes.numel() == 0:
            return torch.tensor(0.0, device=batch_nodes.device), {}
        
        # è·å–èŠ‚ç‚¹ç‰¹å¾
        node_features = self.node_embedding.weight.to(batch_nodes.device) + \
                       self.rel_bias_cache.to(batch_nodes.device)
        
        # è°ƒç”¨GCLæ¨¡å—
        gcl_loss, info_dict = self.gcl_module(
            node_features=node_features,
            full_graph=self.topk_graph,
            rel_embedding=self.rel_embedding,
            batch_node_ids=unique_nodes,
            device=batch_nodes.device
        )
        
        return gcl_loss, info_dict

    def forward(
        self,
        batch_nodes: torch.Tensor,
        batch_lengths,
        semantic: Optional[Dict[str, torch.Tensor]] = None,
        poi: Optional[torch.Tensor] = None,
        traj_poi: Optional[torch.Tensor] = None,
        poi_lengths: Optional[List[int]] = None,
        traj_poi_lengths: Optional[List[int]] = None,
        return_gcl_loss: bool = False,
    ):
        batch_nodes = batch_nodes.to(self.device)
        mask = self._build_padding_mask(batch_lengths, batch_nodes.size(1), device=batch_nodes.device)

        spatial_seq, poi_seq, traj_poi_seq = self._encode_spatial(
            batch_nodes, batch_lengths, poi, traj_poi, mask,
            poi_lengths=poi_lengths, traj_poi_lengths=traj_poi_lengths,
        )
        spatial_seq = self.spatial_pos_enc(spatial_seq)

        if semantic is None:
            semantic = self._semantic_from_nodes(batch_nodes, batch_lengths)

        tokens = semantic["tokens"].to(batch_nodes.device)
        segments = semantic["segments"].to(batch_nodes.device)
        positions = semantic["positions"].to(batch_nodes.device)
        sem_mask = semantic.get("padding_mask", mask).to(batch_nodes.device)

        positions = positions.clamp(max=self.pos_embedding.num_embeddings - 1)

        sem_input = self.token_embedding(tokens) + self.segment_embedding(segments)
        sem_input = sem_input + self.pos_embedding(positions)

        semantic_out = self.semantic_encoder(sem_input, src_key_padding_mask=sem_mask)

        sem_attn, _ = self.co_attn_sem(
            semantic_out, spatial_seq, spatial_seq, key_padding_mask=mask,
        )
        spa_attn, _ = self.co_attn_spa(
            spatial_seq, semantic_out, semantic_out, key_padding_mask=sem_mask,
        )

        combined = torch.cat([sem_attn, spa_attn], dim=-1)
        weighted = self.w_sem * sem_attn + self.w_spa * spa_attn

        fused = self.fuse_linear(combined)
        fused = self.fuse_norm(fused + self.fuse_dropout(weighted))

        ffn_out = self.fusion_ffn(fused)
        fused = self.fusion_ffn_norm(fused + ffn_out)

        traj_repr = self._masked_avg_pool(fused, mask)

        poi_emb_pooled = None
        traj_poi_emb_pooled = None

        if poi_seq is not None:
            if poi_lengths is None:
                poi_mask = mask
            else:
                poi_mask = self._build_padding_mask(poi_lengths, poi_seq.size(1), device=poi_seq.device)
            poi_emb_pooled = self._masked_avg_pool(poi_seq, poi_mask)

        if traj_poi_seq is not None:
            if traj_poi_lengths is None:
                traj_poi_mask = mask
            else:
                traj_poi_mask = self._build_padding_mask(traj_poi_lengths, traj_poi_seq.size(1), device=traj_poi_seq.device)
            traj_poi_emb_pooled = self._masked_avg_pool(traj_poi_seq, traj_poi_mask)

        output = {
            "traj_repr": traj_repr,
            "poi_emb": poi_emb_pooled,
            "traj_poi_emb": traj_poi_emb_pooled,
            "fused_seq": fused,
            "mask": mask,
        }
        
        # ğŸ”¥ å¦‚æœéœ€è¦,è®¡ç®—GCLæŸå¤±
        if return_gcl_loss:
            gcl_loss, gcl_info = self.compute_gcl_loss(batch_nodes, mask)
            output["gcl_loss"] = gcl_loss
            output["gcl_loss_dict"] = gcl_info
        
        return output
# # ä¸‰è§†å›¾
# import logging
# import os
# from collections import defaultdict
# from typing import Dict, Optional, Tuple, List

# import numpy as np
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from Data_Loader import load_semantic_info


# class PositionalEncoding(nn.Module):
#     def __init__(self, d_model: int, max_len: int = 20000):
#         super().__init__()
#         pe = torch.zeros(max_len, d_model)
#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
#         div_term = torch.exp(
#             torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)
#         )
#         pe[:, 0::2] = torch.sin(position * div_term)
#         pe[:, 1::2] = torch.cos(position * div_term)
#         pe = pe.unsqueeze(0)
#         self.register_buffer("pe", pe)

#     def forward(self, x: torch.Tensor) -> torch.Tensor:
#         time_steps = x.size(1)
#         if time_steps > self.pe.size(1):
#             self._expand_pe(time_steps, x.device)
#         return x + self.pe[:, :time_steps, :]

#     def _expand_pe(self, new_len: int, device):
#         d_model = self.pe.size(2)
#         pe = torch.zeros(new_len, d_model, device=device)
#         position = torch.arange(0, new_len, dtype=torch.float, device=device).unsqueeze(1)
#         div_term = torch.exp(
#             torch.arange(0, d_model, 2, device=device).float() * (-np.log(10000.0) / d_model)
#         )
#         pe[:, 0::2] = torch.sin(position * div_term)
#         pe[:, 1::2] = torch.cos(position * div_term)
#         self.pe = pe.unsqueeze(0)


# class GraphContrastiveModule(nn.Module):
#     """
#     å¤šè§†å›¾å›¾å¯¹æ¯”å­¦ä¹ æ¨¡å—
#     æ ¹æ®å…³ç³»ç±»å‹æ„å»ºä¸åŒçš„å›¾è§†å›¾è¿›è¡Œå¯¹æ¯”å­¦ä¹ 
#     """
#     def __init__(self, latent_dim: int, num_nodes: int, temperature: float = 0.07):
#         super().__init__()
#         self.latent_dim = latent_dim
#         self.num_nodes = num_nodes
#         self.temperature = temperature
        
#         # å…³ç³»åˆ†ç»„å®šä¹‰
#         self.SPATIAL_RELS = [0, 7, 8]      # road, highway, non_highway
#         self.TEMPORAL_RELS = [3, 4, 5, 6]  # weekday, weekend, peak, offpeak
#         self.TRAJ_RELS = [1, 2]            # traj_in, traj_not_in
        
#         # æ¯ä¸ªè§†å›¾çš„æŠ•å½±å¤´ (projection head)
#         self.proj_spatial = nn.Sequential(
#             nn.Linear(latent_dim, latent_dim),
#             nn.ReLU(),
#             nn.Linear(latent_dim, latent_dim // 2)
#         )
#         self.proj_temporal = nn.Sequential(
#             nn.Linear(latent_dim, latent_dim),
#             nn.ReLU(),
#             nn.Linear(latent_dim, latent_dim // 2)
#         )
#         self.proj_traj = nn.Sequential(
#             nn.Linear(latent_dim, latent_dim),
#             nn.ReLU(),
#             nn.Linear(latent_dim, latent_dim // 2)
#         )
        
#     def _build_view_graph(self, topk_graph: Dict, allowed_rels: List[int]) -> Dict:
#         """æ ¹æ®å…è®¸çš„å…³ç³»ç±»å‹è¿‡æ»¤å›¾ç»“æ„"""
#         view_graph = defaultdict(list)
#         for nid, neighbors in topk_graph.items():
#             for nb, rel in neighbors:
#                 if rel in allowed_rels:
#                     view_graph[nid].append((nb, rel))
#         return view_graph
    
#     def aggregate_view(
#         self, 
#         node_features: torch.Tensor,  # [N, D]
#         view_graph: Dict,
#         rel_embedding: nn.Embedding,
#         node_ids: torch.Tensor,       # [B] å½“å‰batchçš„èŠ‚ç‚¹ID
#         device: torch.device
#     ) -> torch.Tensor:
#         """
#         åŸºäºç‰¹å®šè§†å›¾çš„å›¾ç»“æ„èšåˆèŠ‚ç‚¹ç‰¹å¾
#         è¿”å›: [B, D] èšåˆåçš„ç‰¹å¾
#         """
#         B = node_ids.size(0)
#         aggregated = torch.zeros(B, self.latent_dim, device=device)
        
#         for i, nid in enumerate(node_ids.tolist()):
#             neighbors = view_graph.get(nid, [])
#             if not neighbors:
#                 # æ²¡æœ‰é‚»å±…åˆ™ä½¿ç”¨è‡ªèº«ç‰¹å¾
#                 aggregated[i] = node_features[nid]
#                 continue
            
#             # æ”¶é›†é‚»å±…ç‰¹å¾å’Œå…³ç³»åµŒå…¥
#             nb_ids = torch.tensor([nb for nb, _ in neighbors], device=device, dtype=torch.long)
#             rel_ids = torch.tensor([rel for _, rel in neighbors], device=device, dtype=torch.long)
            
#             nb_feats = node_features[nb_ids]  # [K, D]
#             rel_feats = rel_embedding(rel_ids)  # [K, D]
            
#             # å…³ç³»åŠ æƒèšåˆ
#             messages = nb_feats + rel_feats
#             aggregated[i] = messages.mean(dim=0)
        
#         return aggregated
    
#     def compute_contrastive_loss(
#         self, 
#         z1: torch.Tensor,  # [B, D'] è§†å›¾1çš„è¡¨ç¤º
#         z2: torch.Tensor,  # [B, D'] è§†å›¾2çš„è¡¨ç¤º
#     ) -> torch.Tensor:
#         """
#         è®¡ç®—InfoNCEå¯¹æ¯”æŸå¤±
#         åŒä¸€èŠ‚ç‚¹åœ¨ä¸åŒè§†å›¾æ˜¯æ­£æ ·æœ¬,å…¶ä»–èŠ‚ç‚¹æ˜¯è´Ÿæ ·æœ¬
#         """
#         B = z1.size(0)
        
#         # L2å½’ä¸€åŒ–
#         z1 = F.normalize(z1, dim=-1)
#         z2 = F.normalize(z2, dim=-1)
        
#         # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [B, B]
#         sim_matrix = torch.mm(z1, z2.t()) / self.temperature
        
#         # å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬,å…¶ä»–æ˜¯è´Ÿæ ·æœ¬
#         labels = torch.arange(B, device=z1.device)
        
#         # InfoNCEæŸå¤± (åŒå‘)
#         loss_12 = F.cross_entropy(sim_matrix, labels)
#         loss_21 = F.cross_entropy(sim_matrix.t(), labels)
        
#         return (loss_12 + loss_21) / 2
    
#     def forward(
#         self,
#         node_features: torch.Tensor,    # [N, D] æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾
#         topk_graph: Dict,                # å®Œæ•´å›¾ç»“æ„
#         rel_embedding: nn.Embedding,     # å…³ç³»åµŒå…¥
#         batch_node_ids: torch.Tensor,    # [B] å½“å‰batchèŠ‚ç‚¹
#         device: torch.device
#     ) -> Tuple[torch.Tensor, Dict]:
#         """
#         è¿”å›: (total_loss, loss_dict)
#         """
#         # æ„å»ºä¸‰ä¸ªè§†å›¾çš„å›¾ç»“æ„
#         spatial_graph = self._build_view_graph(topk_graph, self.SPATIAL_RELS)
#         temporal_graph = self._build_view_graph(topk_graph, self.TEMPORAL_RELS)
#         traj_graph = self._build_view_graph(topk_graph, self.TRAJ_RELS)
        
#         # åœ¨æ¯ä¸ªè§†å›¾ä¸‹èšåˆç‰¹å¾
#         spatial_agg = self.aggregate_view(node_features, spatial_graph, rel_embedding, batch_node_ids, device)
#         temporal_agg = self.aggregate_view(node_features, temporal_graph, rel_embedding, batch_node_ids, device)
#         traj_agg = self.aggregate_view(node_features, traj_graph, rel_embedding, batch_node_ids, device)
        
#         # æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´
#         z_spatial = self.proj_spatial(spatial_agg)
#         z_temporal = self.proj_temporal(temporal_agg)
#         z_traj = self.proj_traj(traj_agg)
        
#         # è®¡ç®—ä¸‰å¯¹è§†å›¾ä¹‹é—´çš„å¯¹æ¯”æŸå¤±
#         loss_spa_temp = self.compute_contrastive_loss(z_spatial, z_temporal)
#         loss_spa_traj = self.compute_contrastive_loss(z_spatial, z_traj)
#         loss_temp_traj = self.compute_contrastive_loss(z_temporal, z_traj)
        
#         total_loss = (loss_spa_temp + loss_spa_traj + loss_temp_traj) / 3
        
#         loss_dict = {
#             'gcl_spa_temp': float(loss_spa_temp.item()),
#             'gcl_spa_traj': float(loss_spa_traj.item()),
#             'gcl_temp_traj': float(loss_temp_traj.item()),
#         }
        
#         return total_loss, loss_dict


# class GRLSTM(nn.Module):
#     """
#     Dual-branch encoder with Graph Contrastive Learning
#     """

#     def __init__(self, args, device, batch_first=True):
#         super().__init__()
#         self.nodes = args.nodes
#         self.latent_dim = args.latent_dim
#         self.device = device
#         self.batch_first = batch_first
#         self.num_heads = args.num_heads
#         self.topk_neighbors = args.topk_neighbors
#         self.self_loop_rel = 9
#         self.rel_vocab = self.self_loop_rel + 1

#         logging.info("Initializing model: latent_dim=%d", self.latent_dim)

#         # Fixed fusion weights
#         alpha_spa = float(getattr(args, "alpha_spa", 0.3))
#         delta_sem = float(getattr(args, "delta_sem", 0.5))
#         s = max(alpha_spa + delta_sem, 1e-8)
#         w_spa = alpha_spa / s
#         w_sem = delta_sem / s

#         self.register_buffer("w_sem", torch.tensor(w_sem, dtype=torch.float32))
#         self.register_buffer("w_spa", torch.tensor(w_spa, dtype=torch.float32))
#         logging.info("Fixed fusion weights: w_sem=%.4f, w_spa=%.4f", w_sem, w_spa)

#         # Semantic meta
#         category_lookup, subclass_lookup, cat_vocab, sub_vocab = load_semantic_info(
#             args.semantic_file, args.nodes
#         )
#         args.category_vocab = getattr(args, "category_vocab", cat_vocab)
#         args.subclass_vocab = getattr(args, "subclass_vocab", sub_vocab)
#         self.register_buffer("category_lookup", torch.as_tensor(category_lookup, dtype=torch.long))
#         self.register_buffer("subclass_lookup", torch.as_tensor(subclass_lookup, dtype=torch.long))

#         # Spatial branch
#         neighbors_np = np.load(args.poi_file, allow_pickle=True)["neighbors"]
#         self.topk_graph = self._build_topk_graph(args.kg_multi_rel_file, neighbors_np)

#         poi_features = np.load(args.poi_feature_file)
#         self.node_embedding = nn.Embedding.from_pretrained(
#             torch.tensor(poi_features, dtype=torch.float32),
#             freeze=False,
#         )
#         self.rel_embedding = nn.Embedding(self.rel_vocab, self.latent_dim)

#         self._precompute_rel_bias()
#         self.spatial_pos_enc = PositionalEncoding(self.latent_dim, max_len=20000)

#         # Semantic branch
#         self.token_embedding = nn.Embedding(args.subclass_vocab, self.latent_dim, padding_idx=0)
#         self.segment_embedding = nn.Embedding(args.category_vocab, self.latent_dim, padding_idx=0)
#         self.pos_embedding = nn.Embedding(args.max_seq_len, self.latent_dim)
#         self.max_seq_len = args.max_seq_len

#         sem_encoder_layer = nn.TransformerEncoderLayer(
#             d_model=self.latent_dim,
#             nhead=args.num_heads,
#             dim_feedforward=args.trans_ffn_dim,
#             dropout=args.trans_dropout,
#             batch_first=True,
#         )
#         self.semantic_encoder = nn.TransformerEncoder(
#             sem_encoder_layer, num_layers=args.trans_layers
#         )

#         # Dual cross-attn
#         self.co_attn_sem = nn.MultiheadAttention(
#             embed_dim=self.latent_dim,
#             num_heads=getattr(args, "co_heads", args.num_heads),
#             batch_first=True,
#             dropout=args.trans_dropout,
#         )
#         self.co_attn_spa = nn.MultiheadAttention(
#             embed_dim=self.latent_dim,
#             num_heads=getattr(args, "co_heads", args.num_heads),
#             batch_first=True,
#             dropout=args.trans_dropout,
#         )

#         self.fuse_linear = nn.Linear(self.latent_dim * 2, self.latent_dim)
#         self.fuse_norm = nn.LayerNorm(self.latent_dim)
#         self.fuse_dropout = nn.Dropout(args.trans_dropout)

#         # FFN block
#         self.fusion_ffn = nn.Sequential(
#             nn.Linear(self.latent_dim, args.trans_ffn_dim),
#             nn.GELU(),
#             nn.Dropout(args.trans_dropout),
#             nn.Linear(args.trans_ffn_dim, self.latent_dim),
#             nn.Dropout(args.trans_dropout),
#         )
#         self.fusion_ffn_norm = nn.LayerNorm(self.latent_dim)
        
#         # ğŸ”¥ NEW: å›¾å¯¹æ¯”å­¦ä¹ æ¨¡å—
#         self.use_gcl = getattr(args, 'use_gcl', True)
#         if self.use_gcl:
#             gcl_temp = getattr(args, 'gcl_temperature', 0.07)
#             self.gcl_module = GraphContrastiveModule(
#                 self.latent_dim, 
#                 self.nodes, 
#                 temperature=gcl_temp
#             )
#             logging.info("Graph Contrastive Learning enabled (temperature=%.3f)", gcl_temp)

#     def _build_topk_graph(self, kg_multi_rel_file: str, neighbors_np: np.ndarray):
#         graph: Dict[int, List[Tuple[int, int]]] = defaultdict(list)

#         if os.path.exists(kg_multi_rel_file):
#             with open(kg_multi_rel_file, "r") as f:
#                 for line in f:
#                     if not line.strip():
#                         continue
#                     try:
#                         u, v, r = line.strip().split()
#                         u, v, r = int(u), int(v), int(r)
#                     except ValueError:
#                         continue
#                     graph[u].append((v, r))
#                     graph[v].append((u, r))

#         for nid in range(self.nodes):
#             candidates = graph.get(nid, [])
#             for nb in neighbors_np[nid]:
#                 candidates.append((int(nb), 0))

#             seen = set()
#             top = []
#             for nb, rel in candidates:
#                 if nb in seen:
#                     continue
#                 seen.add(nb)
#                 top.append((nb, rel))
#                 if len(top) >= self.topk_neighbors:
#                     break
#             graph[nid] = top
#         return graph

#     def _build_padding_mask(self, lengths, max_len, device: Optional[torch.device] = None):
#         dev = device if device is not None else self.rel_bias_cache.device
#         lens = torch.as_tensor(lengths, device=dev)
#         return torch.arange(max_len, device=dev).unsqueeze(0) >= lens.unsqueeze(1)

#     def _masked_avg_pool(self, seq: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:
#         valid = (~padding_mask).float().unsqueeze(-1)
#         seq = seq * valid
#         denom = valid.sum(dim=1).clamp(min=1.0)
#         return seq.sum(dim=1) / denom

#     def _precompute_rel_bias(self):
#         logging.info("[Model] Precomputing rel_bias for %d nodes...", self.nodes)
#         rel_bias = torch.zeros((self.nodes, self.latent_dim), dtype=torch.float32)

#         with torch.no_grad():
#             rel_w = self.rel_embedding.weight.detach().cpu()
#             for nid in range(self.nodes):
#                 neighbors = self.topk_graph.get(nid, [])
#                 if not neighbors:
#                     continue
#                 rel_ids = [rel for _, rel in neighbors]
#                 if len(rel_ids) > 0:
#                     rel_ids_t = torch.tensor(rel_ids, dtype=torch.long)
#                     rel_bias[nid] = rel_w[rel_ids_t].mean(dim=0)

#         self.register_buffer("rel_bias_cache", rel_bias, persistent=True)
#         assert not self.rel_bias_cache.requires_grad
#         logging.info("[Model] Precomputed rel_bias: shape=%s", rel_bias.shape)

#     def _encode_spatial(
#         self,
#         nodes: torch.Tensor,
#         lengths,
#         poi: Optional[torch.Tensor] = None,
#         traj_poi: Optional[torch.Tensor] = None,
#         mask: Optional[torch.Tensor] = None,
#         poi_lengths: Optional[List[int]] = None,
#         traj_poi_lengths: Optional[List[int]] = None,
#     ):
#         dev = nodes.device

#         if mask is None:
#             mask = self._build_padding_mask(lengths, nodes.size(1), device=dev)

#         valid = nodes.masked_select(~mask)

#         if poi is not None:
#             if poi_lengths is not None:
#                 poi_mask = self._build_padding_mask(poi_lengths, poi.size(1), device=poi.device)
#             else:
#                 poi_mask = torch.zeros_like(poi, dtype=torch.bool, device=poi.device)
#             valid = torch.cat([valid, poi.masked_select(~poi_mask)], dim=0)

#         if traj_poi is not None:
#             if traj_poi_lengths is not None:
#                 traj_mask = self._build_padding_mask(traj_poi_lengths, traj_poi.size(1), device=traj_poi.device)
#             else:
#                 traj_mask = torch.zeros_like(traj_poi, dtype=torch.bool, device=traj_poi.device)
#             valid = torch.cat([valid, traj_poi.masked_select(~traj_mask)], dim=0)

#         unique_nodes = torch.unique(valid)
#         if unique_nodes.numel() == 0:
#             unique_nodes = torch.tensor([0], device=dev, dtype=torch.long)

#         edges: List[Tuple[int, int, int]] = []
#         for nid in unique_nodes.tolist():
#             edges.append((nid, nid, self.self_loop_rel))
#             for nb, rel in self.topk_graph.get(nid, []):
#                 edges.append((nb, nid, rel))

#         if not edges:
#             edges = [(0, 0, self.self_loop_rel)]

#         src = torch.as_tensor([e[0] for e in edges], device=dev, dtype=torch.long)
#         dst = torch.as_tensor([e[1] for e in edges], device=dev, dtype=torch.long)
#         rel = torch.as_tensor([e[2] for e in edges], device=dev, dtype=torch.long)

#         node_feats = self.node_embedding.weight.to(dev) + self.rel_bias_cache.to(dev)
#         rel_emb = self.rel_embedding(rel)
#         messages = node_feats[src] + rel_emb

#         agg = torch.zeros_like(node_feats)
#         agg.index_add_(0, dst, messages)

#         deg = torch.zeros(self.nodes, device=dev)
#         deg.index_add_(0, dst, torch.ones(len(dst), device=dev))
#         deg = deg.clamp(min=1).unsqueeze(-1)
#         agg = agg / deg

#         spatial_features = agg + node_feats
#         spatial_features = F.normalize(spatial_features, p=2, dim=-1) * np.sqrt(self.latent_dim)

#         spatial_seq = spatial_features[nodes]
#         poi_seq = spatial_features[poi] if poi is not None else None
#         traj_poi_seq = spatial_features[traj_poi] if traj_poi is not None else None

#         return spatial_seq, poi_seq, traj_poi_seq, spatial_features

#     def _semantic_from_nodes(self, nodes: torch.Tensor, lengths):
#         positions = torch.arange(nodes.size(1), device=nodes.device).unsqueeze(0).expand(nodes.size(0), -1)
#         return {
#             "tokens": self.subclass_lookup[nodes],
#             "segments": self.category_lookup[nodes],
#             "positions": positions,
#             "padding_mask": self._build_padding_mask(lengths, nodes.size(1), device=nodes.device),
#             "lengths": lengths,
#         }

#     def encode_poi_batch(self, poi_batch: torch.Tensor, poi_lengths):
#         poi_batch = poi_batch.to(self.device)
#         mask = self._build_padding_mask(poi_lengths, poi_batch.size(1), device=poi_batch.device)
#         spatial_seq, _, _, _ = self._encode_spatial(
#             poi_batch, poi_lengths, poi=None, traj_poi=None, mask=mask
#         )
#         spatial_seq = self.spatial_pos_enc(spatial_seq)
#         return self._masked_avg_pool(spatial_seq, mask)
    
#     def compute_graph_contrastive_loss(self, batch_nodes: torch.Tensor, mask: torch.Tensor):
#         """
#         è®¡ç®—å›¾å¯¹æ¯”å­¦ä¹ æŸå¤±
#         batch_nodes: [B, T] å½“å‰batchçš„è½¨è¿¹èŠ‚ç‚¹åºåˆ—
#         mask: [B, T] padding mask
#         """
#         if not self.use_gcl:
#             return torch.tensor(0.0, device=batch_nodes.device), {}
        
#         # æå–batchä¸­çš„å”¯ä¸€èŠ‚ç‚¹
#         valid_nodes = batch_nodes.masked_select(~mask)
#         unique_nodes = torch.unique(valid_nodes)
        
#         if unique_nodes.numel() == 0:
#             return torch.tensor(0.0, device=batch_nodes.device), {}
        
#         # è·å–èŠ‚ç‚¹ç‰¹å¾ (åŒ…å«rel_bias)
#         node_features = self.node_embedding.weight.to(batch_nodes.device) + \
#                        self.rel_bias_cache.to(batch_nodes.device)
        
#         # è°ƒç”¨GCLæ¨¡å—
#         gcl_loss, loss_dict = self.gcl_module(
#             node_features=node_features,
#             topk_graph=self.topk_graph,
#             rel_embedding=self.rel_embedding,
#             batch_node_ids=unique_nodes,
#             device=batch_nodes.device
#         )
        
#         return gcl_loss, loss_dict

#     def forward(
#         self,
#         batch_nodes: torch.Tensor,
#         batch_lengths,
#         semantic: Optional[Dict[str, torch.Tensor]] = None,
#         poi: Optional[torch.Tensor] = None,
#         traj_poi: Optional[torch.Tensor] = None,
#         poi_lengths: Optional[List[int]] = None,
#         traj_poi_lengths: Optional[List[int]] = None,
#         return_gcl_loss: bool = False,  # ğŸ”¥ NEW: æ˜¯å¦è¿”å›GCLæŸå¤±
#     ):
#         batch_nodes = batch_nodes.to(self.device)
#         mask = self._build_padding_mask(batch_lengths, batch_nodes.size(1), device=batch_nodes.device)

#         spatial_seq, poi_seq, traj_poi_seq, node_features = self._encode_spatial(
#             batch_nodes, batch_lengths, poi, traj_poi, mask,
#             poi_lengths=poi_lengths, traj_poi_lengths=traj_poi_lengths,
#         )
#         spatial_seq = self.spatial_pos_enc(spatial_seq)

#         if semantic is None:
#             semantic = self._semantic_from_nodes(batch_nodes, batch_lengths)

#         tokens = semantic["tokens"].to(batch_nodes.device)
#         segments = semantic["segments"].to(batch_nodes.device)
#         positions = semantic["positions"].to(batch_nodes.device)
#         sem_mask = semantic.get("padding_mask", mask).to(batch_nodes.device)

#         positions = positions.clamp(max=self.pos_embedding.num_embeddings - 1)

#         sem_input = self.token_embedding(tokens) + self.segment_embedding(segments)
#         sem_input = sem_input + self.pos_embedding(positions)

#         semantic_out = self.semantic_encoder(sem_input, src_key_padding_mask=sem_mask)

#         sem_attn, _ = self.co_attn_sem(
#             semantic_out, spatial_seq, spatial_seq, key_padding_mask=mask,
#         )
#         spa_attn, _ = self.co_attn_spa(
#             spatial_seq, semantic_out, semantic_out, key_padding_mask=sem_mask,
#         )

#         combined = torch.cat([sem_attn, spa_attn], dim=-1)
#         weighted = self.w_sem * sem_attn + self.w_spa * spa_attn

#         fused = self.fuse_linear(combined)
#         fused = self.fuse_norm(fused + self.fuse_dropout(weighted))

#         ffn_out = self.fusion_ffn(fused)
#         fused = self.fusion_ffn_norm(fused + ffn_out)

#         traj_repr = self._masked_avg_pool(fused, mask)

#         poi_emb_pooled = None
#         traj_poi_emb_pooled = None

#         if poi_seq is not None:
#             if poi_lengths is None:
#                 poi_mask = mask
#             else:
#                 poi_mask = self._build_padding_mask(poi_lengths, poi_seq.size(1), device=poi_seq.device)
#             poi_emb_pooled = self._masked_avg_pool(poi_seq, poi_mask)

#         if traj_poi_seq is not None:
#             if traj_poi_lengths is None:
#                 traj_poi_mask = mask
#             else:
#                 traj_poi_mask = self._build_padding_mask(traj_poi_lengths, traj_poi_seq.size(1), device=traj_poi_seq.device)
#             traj_poi_emb_pooled = self._masked_avg_pool(traj_poi_seq, traj_poi_mask)

#         output = {
#             "traj_repr": traj_repr,
#             "poi_emb": poi_emb_pooled,
#             "traj_poi_emb": traj_poi_emb_pooled,
#             "fused_seq": fused,
#             "mask": mask,
#         }
        
#         # ğŸ”¥ NEW: å¦‚æœéœ€è¦,è®¡ç®—GCLæŸå¤±
#         if return_gcl_loss:
#             gcl_loss, gcl_loss_dict = self.compute_graph_contrastive_loss(batch_nodes, mask)
#             output["gcl_loss"] = gcl_loss
#             output["gcl_loss_dict"] = gcl_loss_dict
        
#         return output