# Trainer.py
import logging
import os
import numpy as np
import torch
import torch.optim as optim
from tqdm import tqdm


class Trainer:
    def __init__(
            self,
            model,
            train_data_loader,
            val_data_loader,
            n_epochs,
            lr,
            save_epoch_int,
            model_folder,
            device,
            grad_accum_steps: int = 1,
    ):
        self.train_data_loader = train_data_loader
        self.val_data_loader = val_data_loader
        self.n_epochs = n_epochs
        self.lr = lr
        self.save_epoch_int = save_epoch_int
        self.model_folder = model_folder
        self.device = device
        self.model = model.to(self.device)
        self.grad_accum_steps = max(1, grad_accum_steps)

        if not os.path.exists(model_folder):
            os.makedirs(model_folder)

        self.optim = optim.Adam(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=lr,
        )

    # -----------------------------
    # helpers: seq -> fixed (å…¼å®¹ [B,T,D] / [B,D])
    # -----------------------------
    @staticmethod
    def _masked_avg_pool(seq: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:
        """
        seq: [B, T, D]
        padding_mask: [B, T], True means padding
        return: [B, D]
        """
        valid = (~padding_mask).float().unsqueeze(-1)  # [B, T, 1]
        seq = seq * valid
        denom = valid.sum(dim=1).clamp(min=1.0)  # [B, 1]
        return seq.sum(dim=1) / denom  # [B, D]

    @staticmethod
    def _to_fixed(x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """
        x could be:
          - [B, D]  -> return as-is
          - [B, T, D] -> masked avg pool
          - None -> None
        """
        if x is None:
            return None
        if x.dim() == 2:
            return x
        if x.dim() == 3:
            return Trainer._masked_avg_pool(x, mask)
        raise ValueError(f"Expected x dim=2 or 3, got {x.shape}")

    @staticmethod
    def _dot_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
        """
        a,b: [B, D]
        return: [B]
        """
        if a is None or b is None:
            return None
        if a.dim() != 2 or b.dim() != 2:
            raise ValueError(f"Expected [B,D], got a={a.shape}, b={b.shape}")
        return (a * b).sum(dim=-1)

    @staticmethod
    def _bpr_loss(pos_score: torch.Tensor, neg_score: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
        """
        BPR: -log(sigmoid(pos-neg))
        """
        if pos_score is None or neg_score is None:
            return None
        sig = (pos_score - neg_score).sigmoid().clamp(min=eps)
        return -(sig.log()).mean()

    def _pass(self, data, train=True):
        (
            batch_x, batch_n, batch_y,
            batch_x_len, batch_n_len, batch_y_len,
            batch_traj_poi_pos, batch_traj_poi_neg,
            poi_pos, poi_neg,
            semantic_anchor, semantic_pos, semantic_neg
        ) = data

        # -------- move tensors --------
        batch_x = batch_x.to(self.device)
        batch_y = batch_y.to(self.device)
        batch_n = batch_n.to(self.device)

        poi_pos = poi_pos.to(self.device)
        poi_neg = poi_neg.to(self.device)
        batch_traj_poi_pos = batch_traj_poi_pos.to(self.device)
        batch_traj_poi_neg = batch_traj_poi_neg.to(self.device)

        # -------- move semantic packs --------
        semantic_anchor = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in semantic_anchor.items()}
        semantic_pos = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in semantic_pos.items()}
        semantic_neg = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in semantic_neg.items()}

        eps = 1e-8

        # ==========================================================
        # 1) traj-level: anchor vs pos/neg  (å›ºå®šå‘é‡å¯¹æ¯”) âœ…
        # ==========================================================
        anchor_out = self.model(batch_x, batch_x_len, semantic_anchor)
        pos_out = self.model(batch_y, batch_y_len, semantic_pos)
        neg_out = self.model(batch_n, batch_n_len, semantic_neg)

        pos_score_1 = self._dot_sim(anchor_out["traj_repr"], pos_out["traj_repr"])
        neg_score_1 = self._dot_sim(anchor_out["traj_repr"], neg_out["traj_repr"])
        loss1 = self._bpr_loss(pos_score_1, neg_score_1, eps=eps)

        if loss1 is None:
            # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿ
            loss1 = torch.tensor(0.0, device=self.device)

        # ==========================================================
        # 2) poi-level / traj-ctx-level:
        #    å…³é”®ä¿®å¤ï¼špoi_pos/poi_negã€traj_poi_pos/neg æ˜¯æŒ‰ anchor è½¨è¿¹ç”Ÿæˆçš„
        #    æ‰€ä»¥ loss2/loss3 åªå›´ç»• anchor æ¥åšå¯¹æ¯”ï¼ˆä¸å†ç”¨ pos_out/neg_out çš„åºåˆ—ï¼‰
        # ==========================================================
        loss2 = torch.tensor(0.0, device=self.device)
        loss3 = torch.tensor(0.0, device=self.device)

        # ---- loss2: anchor_traj_repr vs pooled(poi_pos) / pooled(poi_neg) ----
        try:
            # ç”¨åŒä¸€ä¸ª anchor batch_xï¼Œä½†åˆ†åˆ«å–‚ poi_pos / poi_neg
            out_poi_pos = self.model(
                batch_x, batch_x_len, semantic_anchor,
                poi=poi_pos, traj_poi=None,
                poi_lengths=batch_x_len, traj_poi_lengths=None
            )
            out_poi_neg = self.model(
                batch_x, batch_x_len, semantic_anchor,
                poi=poi_neg, traj_poi=None,
                poi_lengths=batch_x_len, traj_poi_lengths=None
            )

            # æŠŠ poi_emb å˜æˆ [B,D]ï¼ˆå…¼å®¹ model è¿”å›ž [B,D] æˆ– [B,T,D]ï¼‰
            p_poi = self._to_fixed(out_poi_pos.get("poi_emb", None), out_poi_pos["mask"])
            n_poi = self._to_fixed(out_poi_neg.get("poi_emb", None), out_poi_neg["mask"])

            a = anchor_out["traj_repr"]  # [B,D]
            poi_pos_score = self._dot_sim(a, p_poi)
            poi_neg_score = self._dot_sim(a, n_poi)

            l2 = self._bpr_loss(poi_pos_score, poi_neg_score, eps=eps)
            if l2 is not None:
                loss2 = l2
        except Exception as e:
            logging.warning(f"[Trainer] skip loss2 (poi) due to error: {e}")

        # ---- loss3: anchor_traj_repr vs pooled(traj_poi_pos) / pooled(traj_poi_neg) ----
        try:
            out_ctx_pos = self.model(
                batch_x, batch_x_len, semantic_anchor,
                poi=None, traj_poi=batch_traj_poi_pos,
                poi_lengths=None, traj_poi_lengths=batch_x_len
            )
            out_ctx_neg = self.model(
                batch_x, batch_x_len, semantic_anchor,
                poi=None, traj_poi=batch_traj_poi_neg,
                poi_lengths=None, traj_poi_lengths=batch_x_len
            )

            p_ctx = self._to_fixed(out_ctx_pos.get("traj_poi_emb", None), out_ctx_pos["mask"])
            n_ctx = self._to_fixed(out_ctx_neg.get("traj_poi_emb", None), out_ctx_neg["mask"])

            a = anchor_out["traj_repr"]  # [B,D]
            ctx_pos_score = self._dot_sim(a, p_ctx)
            ctx_neg_score = self._dot_sim(a, n_ctx)

            l3 = self._bpr_loss(ctx_pos_score, ctx_neg_score, eps=eps)
            if l3 is not None:
                loss3 = l3
        except Exception as e:
            logging.warning(f"[Trainer] skip loss3 (traj_ctx) due to error: {e}")

        loss = loss1 + loss2 + loss3

        if train:
            torch.backends.cudnn.enabled = False
            (loss / self.grad_accum_steps).backward()

        return float(loss.item())

    def _train_epoch(self):
        self.model.train()
        losses = []
        pbar = tqdm(self.train_data_loader)
        self.optim.zero_grad()
        for step, data in enumerate(pbar, 1):
            loss = self._pass(data, train=True)
            losses.append(loss)
            pbar.set_description("[loss: %f]" % loss)

            if step % self.grad_accum_steps == 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)
                self.optim.step()
                self.optim.zero_grad()

        # flush remaining gradients
        if len(self.train_data_loader) % self.grad_accum_steps != 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)
            self.optim.step()
            self.optim.zero_grad()

        return float(np.array(losses).mean())

    def _val_epoch(self):
        self.model.eval()
        if self.val_data_loader is None:
            return None
        losses = []
        pbar = tqdm(self.val_data_loader)
        with torch.no_grad():
            for data in pbar:
                loss = self._pass(data, train=False)
                losses.append(loss)
                pbar.set_description("[val_loss: %f]" % loss)
        return float(np.array(losses).mean()) if losses else None

    def train(self):
        for epoch in range(self.n_epochs):
            train_loss = self._train_epoch()
            logging.info("[Epoch %d/%d] [training loss: %f]" % (epoch, self.n_epochs, train_loss))

            val_loss = self._val_epoch()
            if val_loss is not None:
                logging.info("[Epoch %d/%d] [val loss: %f]" % (epoch, self.n_epochs, val_loss))

            if (epoch + 1) % self.save_epoch_int == 0:
                save_file = os.path.join(self.model_folder, "epoch_%d.pt" % epoch)
                model_to_save = self.model.module if isinstance(self.model, torch.nn.DataParallel) else self.model
                torch.save(model_to_save.state_dict(), save_file)
                logging.info(f"[Trainer] saved checkpoint -> {save_file}")

## 3. Trainerä»£ç  (`Trainer.py`)
# ```python
# Trainer.py
# import logging
# import os
# import numpy as np
# import torch
# import torch.optim as optim
# from tqdm import tqdm


# class Trainer:
#     def __init__(
#         self,
#         model,
#         train_data_loader,
#         val_data_loader,
#         n_epochs,
#         lr,
#         save_epoch_int,
#         model_folder,
#         device,
#     ):
#         self.train_data_loader = train_data_loader
#         self.val_data_loader = val_data_loader
#         self.n_epochs = n_epochs
#         self.lr = lr
#         self.save_epoch_int = save_epoch_int
#         self.model_folder = model_folder
#         self.device = device
#         self.model = model.to(self.device)

#         if not os.path.exists(model_folder):
#             os.makedirs(model_folder)

#         self.optim = optim.Adam(
#             filter(lambda p: p.requires_grad, self.model.parameters()),
#             lr=lr,
#         )

#         # ðŸ”¥ ä¼˜åŒ–2ï¼šåªåœ¨åˆå§‹åŒ–æ—¶è®°å½•CuDNNçŠ¶æ€ï¼ˆä¸å†æ¯stepä¿®æ”¹ï¼‰
#         logging.info(f"[Trainer] CuDNN enabled: {torch.backends.cudnn.enabled}")
#         logging.info(f"[Trainer] CuDNN deterministic: {torch.backends.cudnn.deterministic}")

#     # -----------------------------
#     # helpers
#     # -----------------------------
#     @staticmethod
#     def _masked_avg_pool(seq: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:
#         """
#         seq: [B, T, D]
#         padding_mask: [B, T], True means padding
#         return: [B, D]
#         """
#         valid = (~padding_mask).float().unsqueeze(-1)  # [B, T, 1]
#         seq = seq * valid
#         denom = valid.sum(dim=1).clamp(min=1.0)        # [B, 1]
#         return seq.sum(dim=1) / denom                  # [B, D]

#     @staticmethod
#     def _to_fixed(x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
#         """
#         âœ… ä¿®å¤ï¼šå®‰å…¨å¤„ç†maskç»´åº¦ä¸åŒ¹é…
#         x could be:
#           - [B, D]  -> return as-is
#           - [B, T, D] -> masked avg pool
#           - None -> None
#         """
#         if x is None:
#             return None
#         if x.dim() == 2:
#             return x
#         if x.dim() == 3:
#             # å¦‚æžœmaskä¸åŒ¹é…æˆ–ä¸ºNoneï¼Œåˆ›å»ºå…¨Falseçš„maskï¼ˆå…¨éƒ¨æœ‰æ•ˆï¼‰
#             if mask is None or mask.size(1) != x.size(1):
#                 mask = torch.zeros(x.size(0), x.size(1),
#                                   dtype=torch.bool, device=x.device)
#             return Trainer._masked_avg_pool(x, mask)
#         raise ValueError(f"Expected x dim=2 or 3, got {x.shape}")

#     @staticmethod
#     def _dot_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
#         """
#         a,b: [B, D]
#         return: [B]
#         """
#         if a is None or b is None:
#             return None
#         if a.dim() != 2 or b.dim() != 2:
#             raise ValueError(f"Expected [B,D], got a={a.shape}, b={b.shape}")
#         return (a * b).sum(dim=-1)

#     @staticmethod
#     def _bpr_loss(pos_score: torch.Tensor, neg_score: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
#         """
#         BPR: -log(sigmoid(pos-neg))
#         """
#         if pos_score is None or neg_score is None:
#             return None
#         sig = (pos_score - neg_score).sigmoid().clamp(min=eps)
#         return -(sig.log()).mean()

#     def _pass(self, data, train=True):
#         self.optim.zero_grad()

#         (
#             batch_x, batch_n, batch_y,
#             batch_x_len, batch_n_len, batch_y_len,
#             batch_traj_poi_pos, batch_traj_poi_neg,
#             poi_pos, poi_neg,
#             semantic_anchor, semantic_pos, semantic_neg
#         ) = data

#         # -------- move tensors --------
#         batch_x = batch_x.to(self.device)
#         batch_y = batch_y.to(self.device)
#         batch_n = batch_n.to(self.device)

#         poi_pos = poi_pos.to(self.device)
#         poi_neg = poi_neg.to(self.device)
#         batch_traj_poi_pos = batch_traj_poi_pos.to(self.device)
#         batch_traj_poi_neg = batch_traj_poi_neg.to(self.device)

#         # -------- move semantic packs --------
#         semantic_anchor = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in semantic_anchor.items()}
#         semantic_pos    = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in semantic_pos.items()}
#         semantic_neg    = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in semantic_neg.items()}

#         eps = 1e-8

#         # ==========================================================
#         # 1) traj-level: anchor vs pos/neg âœ…
#         # ==========================================================
#         anchor_out = self.model(batch_x, batch_x_len, semantic_anchor)
#         pos_out = self.model(batch_y, batch_y_len, semantic_pos)
#         neg_out = self.model(batch_n, batch_n_len, semantic_neg)

#         pos_score_1 = self._dot_sim(anchor_out["traj_repr"], pos_out["traj_repr"])
#         neg_score_1 = self._dot_sim(anchor_out["traj_repr"], neg_out["traj_repr"])
#         loss1 = self._bpr_loss(pos_score_1, neg_score_1, eps=eps)

#         if loss1 is None:
#             loss1 = torch.tensor(0.0, device=self.device)

#         # ==========================================================
#         # 2) poi-level: âœ… ä½¿ç”¨æ–°çš„encode_poi_batchæ–¹æ³•
#         # ==========================================================
#         loss2 = torch.tensor(0.0, device=self.device)
#         try:
#             # ç›´æŽ¥ç¼–ç poi_pos/negï¼ˆé¿å…é‡å¤forwardï¼‰
#             p_poi = self.model.encode_poi_batch(poi_pos, batch_x_len)
#             n_poi = self.model.encode_poi_batch(poi_neg, batch_x_len)

#             a = anchor_out["traj_repr"]  # [B,D]
#             poi_pos_score = self._dot_sim(a, p_poi)
#             poi_neg_score = self._dot_sim(a, n_poi)

#             l2 = self._bpr_loss(poi_pos_score, poi_neg_score, eps=eps)
#             if l2 is not None:
#                 loss2 = l2
#         except Exception as e:
#             logging.warning(f"[Trainer] skip loss2 (poi) due to error: {e}")

#         # ==========================================================
#         # 3) traj-ctx-level: âœ… ä½¿ç”¨æ–°çš„encode_poi_batchæ–¹æ³•
#         # ==========================================================
#         loss3 = torch.tensor(0.0, device=self.device)
#         try:
#             # ç›´æŽ¥ç¼–ç traj_poi_pos/neg
#             p_ctx = self.model.encode_poi_batch(batch_traj_poi_pos, batch_x_len)
#             n_ctx = self.model.encode_poi_batch(batch_traj_poi_neg, batch_x_len)

#             a = anchor_out["traj_repr"]  # [B,D]
#             ctx_pos_score = self._dot_sim(a, p_ctx)
#             ctx_neg_score = self._dot_sim(a, n_ctx)

#             l3 = self._bpr_loss(ctx_pos_score, ctx_neg_score, eps=eps)
#             if l3 is not None:
#                 loss3 = l3
#         except Exception as e:
#             logging.warning(f"[Trainer] skip loss3 (traj_ctx) due to error: {e}")

#         # total loss
#         loss = loss1 + loss2 + loss3

#         if train:
#             # âœ… ä¼˜åŒ–2ï¼šç§»é™¤cudnnä¿®æ”¹ï¼Œç›´æŽ¥è®­ç»ƒ
#             loss.backward()
#             torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)
#             self.optim.step()

#         return float(loss.item())

#     def _train_epoch(self):
#         self.model.train()
#         losses = []
#         pbar = tqdm(self.train_data_loader)
#         for data in pbar:
#             loss = self._pass(data, train=True)
#             losses.append(loss)
#             pbar.set_description("[loss: %f]" % loss)
#         return float(np.array(losses).mean())

#     def _val_epoch(self):
#         self.model.eval()
#         if self.val_data_loader is None:
#             return None
#         losses = []
#         pbar = tqdm(self.val_data_loader)
#         with torch.no_grad():
#             for data in pbar:
#                 loss = self._pass(data, train=False)
#                 losses.append(loss)
#                 pbar.set_description("[val_loss: %f]" % loss)
#         return float(np.array(losses).mean()) if losses else None

#     def train(self):
#         for epoch in range(self.n_epochs):
#             train_loss = self._train_epoch()
#             logging.info("[Epoch %d/%d] [training loss: %f]" % (epoch, self.n_epochs, train_loss))

#             val_loss = self._val_epoch()
#             if val_loss is not None:
#                 logging.info("[Epoch %d/%d] [val loss: %f]" % (epoch, self.n_epochs, val_loss))

#             if (epoch + 1) % self.save_epoch_int == 0:
#                 save_file = os.path.join(self.model_folder, "epoch_%d.pt" % epoch)
#                 torch.save(self.model.state_dict(), save_file)
#                 logging.info(f"[Trainer] saved checkpoint -> {save_file}")

# ç‰ˆæœ¬1---------------------------------------
# # Trainer.py
# import logging
# import os
# import numpy as np
# import torch
# import torch.optim as optim
# from tqdm import tqdm


# class Trainer:
#     def __init__(
#         self,
#         model,
#         train_data_loader,
#         val_data_loader,
#         n_epochs,
#         lr,
#         save_epoch_int,
#         model_folder,
#         device,
#     ):
#         self.train_data_loader = train_data_loader
#         self.val_data_loader = val_data_loader
#         self.n_epochs = n_epochs
#         self.lr = lr
#         self.save_epoch_int = save_epoch_int
#         self.model_folder = model_folder
#         self.device = device
#         self.model = model.to(self.device)

#         if not os.path.exists(model_folder):
#             os.makedirs(model_folder)

#         self.optim = optim.Adam(
#             filter(lambda p: p.requires_grad, self.model.parameters()),
#             lr=lr,
#         )

#     # -----------------------------
#     # helpers: seq -> fixed (å…¼å®¹ [B,T,D] / [B,D])
#     # -----------------------------
#     @staticmethod
#     def _masked_avg_pool(seq: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:
#         """
#         seq: [B, T, D]
#         padding_mask: [B, T], True means padding
#         return: [B, D]
#         """
#         valid = (~padding_mask).float().unsqueeze(-1)  # [B, T, 1]
#         seq = seq * valid
#         denom = valid.sum(dim=1).clamp(min=1.0)        # [B, 1]
#         return seq.sum(dim=1) / denom                  # [B, D]

#     @staticmethod
#     def _to_fixed(x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
#         """
#         x could be:
#           - [B, D]  -> return as-is
#           - [B, T, D] -> masked avg pool
#           - None -> None
#         """
#         if x is None:
#             return None
#         if x.dim() == 2:
#             return x
#         if x.dim() == 3:
#             return Trainer._masked_avg_pool(x, mask)
#         raise ValueError(f"Expected x dim=2 or 3, got {x.shape}")

#     @staticmethod
#     def _dot_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
#         """
#         a,b: [B, D]
#         return: [B]
#         """
#         if a is None or b is None:
#             return None
#         if a.dim() != 2 or b.dim() != 2:
#             raise ValueError(f"Expected [B,D], got a={a.shape}, b={b.shape}")
#         return (a * b).sum(dim=-1)

#     @staticmethod
#     def _bpr_loss(pos_score: torch.Tensor, neg_score: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
#         """
#         BPR: -log(sigmoid(pos-neg))
#         """
#         if pos_score is None or neg_score is None:
#             return None
#         sig = (pos_score - neg_score).sigmoid().clamp(min=eps)
#         return -(sig.log()).mean()

#     def _pass(self, data, train=True):
#         self.optim.zero_grad()

#         (
#             batch_x, batch_n, batch_y,
#             batch_x_len, batch_n_len, batch_y_len,
#             batch_traj_poi_pos, batch_traj_poi_neg,
#             poi_pos, poi_neg,
#             semantic_anchor, semantic_pos, semantic_neg
#         ) = data

#         # -------- move tensors --------
#         batch_x = batch_x.to(self.device)
#         batch_y = batch_y.to(self.device)
#         batch_n = batch_n.to(self.device)

#         poi_pos = poi_pos.to(self.device)
#         poi_neg = poi_neg.to(self.device)
#         batch_traj_poi_pos = batch_traj_poi_pos.to(self.device)
#         batch_traj_poi_neg = batch_traj_poi_neg.to(self.device)

#         # -------- move semantic packs --------
#         semantic_anchor = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in semantic_anchor.items()}
#         semantic_pos    = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in semantic_pos.items()}
#         semantic_neg    = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in semantic_neg.items()}

#         eps = 1e-8

#         # ==========================================================
#         # 1) traj-level: anchor vs pos/neg  (å›ºå®šå‘é‡å¯¹æ¯”) âœ…
#         # ==========================================================
#         anchor_out = self.model(batch_x, batch_x_len, semantic_anchor)
#         pos_out = self.model(batch_y, batch_y_len, semantic_pos)
#         neg_out = self.model(batch_n, batch_n_len, semantic_neg)

#         pos_score_1 = self._dot_sim(anchor_out["traj_repr"], pos_out["traj_repr"])
#         neg_score_1 = self._dot_sim(anchor_out["traj_repr"], neg_out["traj_repr"])
#         loss1 = self._bpr_loss(pos_score_1, neg_score_1, eps=eps)

#         if loss1 is None:
#             # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿ
#             loss1 = torch.tensor(0.0, device=self.device)

#         # ==========================================================
#         # 2) poi-level / traj-ctx-level:
#         #    å…³é”®ä¿®å¤ï¼špoi_pos/poi_negã€traj_poi_pos/neg æ˜¯æŒ‰ anchor è½¨è¿¹ç”Ÿæˆçš„
#         #    æ‰€ä»¥ loss2/loss3 åªå›´ç»• anchor æ¥åšå¯¹æ¯”ï¼ˆä¸å†ç”¨ pos_out/neg_out çš„åºåˆ—ï¼‰
#         # ==========================================================
#         loss2 = torch.tensor(0.0, device=self.device)
#         loss3 = torch.tensor(0.0, device=self.device)

#         # ---- loss2: anchor_traj_repr vs pooled(poi_pos) / pooled(poi_neg) ----
#         try:
#             # ç”¨åŒä¸€ä¸ª anchor batch_xï¼Œä½†åˆ†åˆ«å–‚ poi_pos / poi_neg
#             out_poi_pos = self.model(
#                 batch_x, batch_x_len, semantic_anchor,
#                 poi=poi_pos, traj_poi=None,
#                 poi_lengths=batch_x_len, traj_poi_lengths=None
#             )
#             out_poi_neg = self.model(
#                 batch_x, batch_x_len, semantic_anchor,
#                 poi=poi_neg, traj_poi=None,
#                 poi_lengths=batch_x_len, traj_poi_lengths=None
#             )

#             # æŠŠ poi_emb å˜æˆ [B,D]ï¼ˆå…¼å®¹ model è¿”å›ž [B,D] æˆ– [B,T,D]ï¼‰
#             p_poi = self._to_fixed(out_poi_pos.get("poi_emb", None), out_poi_pos["mask"])
#             n_poi = self._to_fixed(out_poi_neg.get("poi_emb", None), out_poi_neg["mask"])

#             a = anchor_out["traj_repr"]  # [B,D]
#             poi_pos_score = self._dot_sim(a, p_poi)
#             poi_neg_score = self._dot_sim(a, n_poi)

#             l2 = self._bpr_loss(poi_pos_score, poi_neg_score, eps=eps)
#             if l2 is not None:
#                 loss2 = l2
#         except Exception as e:
#             logging.warning(f"[Trainer] skip loss2 (poi) due to error: {e}")

#         # ---- loss3: anchor_traj_repr vs pooled(traj_poi_pos) / pooled(traj_poi_neg) ----
#         try:
#             out_ctx_pos = self.model(
#                 batch_x, batch_x_len, semantic_anchor,
#                 poi=None, traj_poi=batch_traj_poi_pos,
#                 poi_lengths=None, traj_poi_lengths=batch_x_len
#             )
#             out_ctx_neg = self.model(
#                 batch_x, batch_x_len, semantic_anchor,
#                 poi=None, traj_poi=batch_traj_poi_neg,
#                 poi_lengths=None, traj_poi_lengths=batch_x_len
#             )

#             p_ctx = self._to_fixed(out_ctx_pos.get("traj_poi_emb", None), out_ctx_pos["mask"])
#             n_ctx = self._to_fixed(out_ctx_neg.get("traj_poi_emb", None), out_ctx_neg["mask"])

#             a = anchor_out["traj_repr"]  # [B,D]
#             ctx_pos_score = self._dot_sim(a, p_ctx)
#             ctx_neg_score = self._dot_sim(a, n_ctx)

#             l3 = self._bpr_loss(ctx_pos_score, ctx_neg_score, eps=eps)
#             if l3 is not None:
#                 loss3 = l3
#         except Exception as e:
#             logging.warning(f"[Trainer] skip loss3 (traj_ctx) due to error: {e}")

#         # total loss
#         loss = loss1 + loss2 + loss3

#         if train:
#             # ä½ åŽŸæ¥å…³ cudnn æ˜¯ä¸ºäº†æŸäº›å¯å¤çŽ°/å¯¹é½é—®é¢˜ï¼›ä¿ç•™ä¹Ÿè¡Œ
#             torch.backends.cudnn.enabled = False
#             loss.backward()
#             torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)
#             self.optim.step()

#         return float(loss.item())

#     def _train_epoch(self):
#         self.model.train()
#         losses = []
#         pbar = tqdm(self.train_data_loader)
#         for data in pbar:
#             loss = self._pass(data, train=True)
#             losses.append(loss)
#             pbar.set_description("[loss: %f]" % loss)
#         return float(np.array(losses).mean())

#     def _val_epoch(self):
#         self.model.eval()
#         if self.val_data_loader is None:
#             return None
#         losses = []
#         pbar = tqdm(self.val_data_loader)
#         with torch.no_grad():
#             for data in pbar:
#                 loss = self._pass(data, train=False)
#                 losses.append(loss)
#                 pbar.set_description("[val_loss: %f]" % loss)
#         return float(np.array(losses).mean()) if losses else None

#     def train(self):
#         for epoch in range(self.n_epochs):
#             train_loss = self._train_epoch()
#             logging.info("[Epoch %d/%d] [training loss: %f]" % (epoch, self.n_epochs, train_loss))

#             val_loss = self._val_epoch()
#             if val_loss is not None:
#                 logging.info("[Epoch %d/%d] [val loss: %f]" % (epoch, self.n_epochs, val_loss))

#             if (epoch + 1) % self.save_epoch_int == 0:
#                 save_file = os.path.join(self.model_folder, "epoch_%d.pt" % epoch)
#                 torch.save(self.model.state_dict(), save_file)
#                 logging.info(f"[Trainer] saved checkpoint -> {save_file}")
